{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some basic imports and setups\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "\n",
    "#mean of imagenet dataset in BGR\n",
    "imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "image_dir = os.path.join(current_dir, 'memes')\n",
    "#image_dir = current_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "\n",
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000,[],['fc7','fc8'],512,weights_path='/data/alpv95/MemeProject/im2txt/bvlc_alexnet.npy') #maybe need to put fc8 in skip_layers\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc6\n",
    "\n",
    "#create op to calculate softmax \n",
    "#softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Converting captions and meme vector representations into single Tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires putting memes through alexnet to find their vector rep, shuffling the captions, changing captions into their word2idx, finally saving one caption together with one meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194, 194)\n",
      "(100, 19500, 19500, 19500)\n",
      "sizing error\n",
      "(200, 38642, 38642, 38642)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(300, 57715, 57715, 57715)\n",
      "sizing error\n",
      "sizing error\n",
      "(400, 76329, 76329, 76329)\n",
      "sizing error\n",
      "sizing error\n",
      "(500, 94591, 94591, 94591)\n",
      "sizing error\n",
      "sizing error\n",
      "(600, 113146, 113146, 113146)\n",
      "sizing error\n",
      "(700, 131507, 131507, 131507)\n",
      "sizing error\n",
      "(800, 149520, 149520, 149520)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(900, 167518, 167518, 167518)\n",
      "(1000, 185425, 185425, 185425)\n",
      "sizing error\n",
      "(1100, 203294, 203294, 203294)\n",
      "(1200, 220336, 220336, 220336)\n",
      "sizing error\n",
      "(1300, 237823, 237823, 237823)\n",
      "sizing error\n",
      "sizing error\n",
      "(1400, 254129, 254129, 254129)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(1500, 270058, 270058, 270058)\n",
      "(1600, 286622, 286622, 286622)\n",
      "sizing error\n",
      "sizing error\n",
      "(1700, 302200, 302200, 302200)\n",
      "sizing error\n",
      "(1800, 316652, 316652, 316652)\n",
      "sizing error\n",
      "(1900, 331886, 331886, 331886)\n",
      "(2000, 346319, 346319, 346319)\n",
      "sizing error\n",
      "sizing error\n",
      "(2100, 362089, 362089, 362089)\n",
      "(2200, 375126, 375126, 375126)\n",
      "sizing error\n",
      "sizing error\n",
      "(2300, 389123, 389123, 389123)\n",
      "sizing error\n",
      "sizing error\n",
      "(2400, 401660, 401660, 401660)\n",
      "(2500, 414389, 414389, 414389)\n",
      "23\n",
      "414389\n"
     ]
    }
   ],
   "source": [
    "#USE FOR ALEXNET !!!!!!!!!\n",
    "#TRAINING SET\n",
    "\n",
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "data_meme_names = [] #just to check captions have been paired correctly\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "#Doing everything in one script: (the fc6 vectors are quite sparse)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    for i,meme in enumerate(img_files):\n",
    "        #meme_name = meme.replace('/Users/ALP/Desktop/Stanford/CS224n/MemeProject/memes/','')\n",
    "        #meme_name = meme_name.replace('.jpg','').lower()\n",
    "        #meme_name = meme_name.replace('-',' ')\n",
    "        img = Image.open(meme)\n",
    "        try:\n",
    "            img.thumbnail((227, 227), Image.ANTIALIAS)\n",
    "            #img = img.resize((227,227))\n",
    "            #use img.thumbnail for square images, img.resize for non square\n",
    "            assert np.shape(img) == (227, 227, 3)\n",
    "        except AssertionError:\n",
    "            img = img.resize((227,227))\n",
    "            print('sizing error')\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img = img - imagenet_mean #should probably change this\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "\n",
    "        meme_vector = sess.run(score, feed_dict={x: img, keep_prob: 1}) #[1,4096]\n",
    "        meme_vector = np.reshape(meme_vector,[4096])\n",
    "        assert np.shape(meme_vector) == (4096,)\n",
    "        \n",
    "        match = []\n",
    "        meme_name = captions[counter].split(' - ')[0]\n",
    "        image_name = img_files[i].replace('/data/alpv95/MemeProject/im2txt/memes/','')\n",
    "        image_name = image_name.replace('.jpg','')\n",
    "        image_name = image_name.replace('-',' ')\n",
    "        #print(meme_name,image_name)\n",
    "        try:\n",
    "            assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "        except AssertionError:\n",
    "            passed+=1\n",
    "            continue\n",
    "        \n",
    "        while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "            if counter==len(captions)-1:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                break\n",
    "            elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "                counter += 1\n",
    "            else:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                counter += 1\n",
    "                \n",
    "        \n",
    "        #now save in tfrecords format, or prepare for that action\n",
    "        meme_vectors = [meme_vector for cap in match]\n",
    "        image_names = [image_name for cap in match]\n",
    "        assert len(meme_vectors) == len(match)\n",
    "        data_memes.extend(meme_vectors)\n",
    "        data_captions.extend(match)\n",
    "        data_meme_names.extend(image_names)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i,len(data_memes),len(data_captions),len(data_meme_names))\n",
    "\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194)\n",
      "(100, 19500, 19500)\n",
      "(200, 38642, 38642)\n",
      "(300, 57715, 57715)\n",
      "(400, 76329, 76329)\n",
      "(500, 94591, 94591)\n",
      "(600, 113146, 113146)\n",
      "(700, 131507, 131507)\n",
      "(800, 149520, 149520)\n",
      "(900, 167518, 167518)\n",
      "(1000, 185425, 185425)\n",
      "(1100, 203294, 203294)\n",
      "(1200, 220336, 220336)\n",
      "(1300, 237823, 237823)\n",
      "(1400, 254129, 254129)\n",
      "(1500, 270058, 270058)\n",
      "(1600, 286622, 286622)\n",
      "(1700, 302200, 302200)\n",
      "(1800, 316652, 316652)\n",
      "(1900, 331886, 331886)\n",
      "(2000, 346319, 346319)\n",
      "(2100, 362089, 362089)\n",
      "(2200, 375126, 375126)\n",
      "(2300, 389123, 389123)\n",
      "(2400, 401660, 401660)\n",
      "(2500, 414389, 414389)\n",
      "23\n",
      "414389\n"
     ]
    }
   ],
   "source": [
    "#USE FOR INCEPTION NETWORK (OR ALEXNET WITH FINETUNING)\n",
    "#TRAINING SET\n",
    "#For this case just need to save image filenames alongside captions\n",
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "for i,meme in enumerate(img_files):\n",
    "    \n",
    "    match = []\n",
    "    meme_name = captions[counter].split(' - ')[0]\n",
    "    image_name = meme.replace('/data/alpv95/MemeProject/im2txt/memes/','')\n",
    "    image_name = image_name.replace('.jpg','')\n",
    "    image_name = image_name.replace('-',' ')\n",
    "    #print(meme_name,image_name)\n",
    "    try:\n",
    "        assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "    except AssertionError:\n",
    "        passed+=1\n",
    "        continue\n",
    "        \n",
    "    while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "        if counter==len(captions)-1:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            break\n",
    "        elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "            counter += 1\n",
    "        else:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            counter += 1\n",
    "                \n",
    "        \n",
    "    #now save in tfrecords format, or prepare for that action\n",
    "    meme_images = [meme for cap in match]\n",
    "    assert len(meme_images) == len(match)\n",
    "    data_memes.extend(meme_images)\n",
    "    data_captions.extend(match)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i,len(data_memes),len(data_captions))\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.376192  0.        0.       ...  0.        0.        0.      ]\n",
      "i am the definition of  thundercunt\n",
      "\n",
      "[21.376192  0.        0.       ...  0.        0.        0.      ]\n",
      "you had delicious trisect crackers and didn't tell me?!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,102):\n",
    "    print(data_memes[i+402000])\n",
    "    print(data_captions[i+402000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'omg look at her butt\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_captions[len(data_memes)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y u no', 'meme generator users y u no give me more upvotes?\\n', 0)\n",
      "('y u no', 'y you no studying\\n', 190)\n",
      "('willy wonka', 'so you are in love? who is it this week?\\n', 380)\n",
      "('the most interesting man in the world', 'i dont always have sex but when i do its with your girlfriend\\n', 570)\n",
      "('futurama fry', 'not sure if beef or horse meat\\n', 760)\n",
      "('success kid', 'windows checking for a solution actually found a solution\\n', 950)\n",
      "('one does not simply', 'one does not simply watch all 3 movies in a row\\n', 1140)\n",
      "('bad luck brian', 'gets beat up in school suspended for \"involvement\"\\n', 1330)\n",
      "('first world problems', 'bit into a jelly donut got a dry piece\\n', 1520)\n",
      "('philosoraptor', \"if a there's a zombie outbreak in vegas   does it stay in vegas?\\n\", 1710)\n",
      "('grumpy cat', \"sure i'll play with you... how about russian roulette\\n\", 1900)\n",
      "('winter is coming', 'brace yourself walter is coming... and he is going to kick some ass\\n', 2090)\n",
      "('forever alone', 'happy birthday from geico insurance\\n', 2280)\n",
      "('good guy greg', 'no pics its cool i believe it happened\\n', 2470)\n",
      "('scumbag steve', \"makes fun of you for moving into a trailer live in his mom's basement\\n\", 2660)\n",
      "('what if i told you', \"what if i told you your thought process isn't directly linked to your skin color?\\n\", 2850)\n",
      "('conspiracy keanu', 'what if when you walk into a room and forget what you were doing god is playing the sims and gust canceled your action\\n', 3040)\n",
      "('kermit the frog drinking tea', \"i saw you pull that foodstamp card out your gucci bag but that's none of my business tho\\n\", 3230)\n",
      "('yo dawg', 'yo dawg we heard you like call of duty so we sent you to afghanistan\\n', 3420)\n",
      "('all the things', 'bathe all the monkeys\\n', 3610)\n",
      "('insanity wolf', \"sexy local singles don't distract my fapping\\n\", 3800)\n",
      "('joseph ducreux', 'shall i halt or shall i make haste\\n', 3990)\n",
      "('pedobear', 'bought extra freezers for his ice cream truck still never has ice cream\\n', 4180)\n",
      "('trollface', 'hey guys see what happens when hit the caps lock key and press like\\n', 4370)\n",
      "('annoying facebook girl', 'i hate attention seeking girls \"i\\'m so ugly\" on every photo\\n', 4560)\n",
      "('socially awkward penguin', \"make perfect meme real life friends don't understand and don't care\\n\", 4750)\n",
      "('skeptical 3rd world kid', \"so you're telling me gaining weight is a bad thing\\n\", 4940)\n",
      "('disaster girl', \"they didn't attend my birthday party? i didn't attend their funeral\\n\", 5130)\n",
      "('prepare yourself', 'prepare yourselves  new years resolutions are coming \\n', 5320)\n",
      "('slowpoke', 'hey everybody! they finally got osama!\\n', 5510)\n",
      "('dr evil meme', 'you have a \"beard\"\\n', 5700)\n",
      "('advice yoda gives', 'real shit just got.\\n', 5890)\n",
      "('joker mind loss', 'blow all over a birthday cake and no one blinks an eye sneeze near food and everyone lose their minds\\n', 6080)\n",
      "('stoner stanley', 'i would rather die than commit suicide\\n', 6270)\n",
      "('pleaseguy', 'friends please stop having wet dreams during sleepovers\\n', 6460)\n",
      "('foul bachelor frog', 'feminine hygiene jokes are the lowest form of humor period\\n', 6650)\n",
      "('high expectations asian father', 'what is difference between a- and a+? my love for you\\n', 6840)\n",
      "('koala cant believe it', 'you mean, my calculator has games on it?\\n', 7030)\n",
      "('batman slap robin', \"but god said love thy... he didn't mean mexico!!\\n\", 7220)\n",
      "('mr bean', 'i came...\\n', 7410)\n",
      "('chuck norris', \"the sun doesn't set it just hides from chuck norris\\n\", 7600)\n",
      "('overly attached girlfriend', 'from now on introduce me as your soul mate\\n', 7790)\n",
      "('butthurt dweller', 'takes bibles moves them to the fiction section\\n', 7980)\n",
      "('desk flip rage guy', 'when a gym leader uses full restore on pokemon!\\n', 8170)\n",
      "('drunk baby 1', \"you'd be wise to listen to my words, nigel just doing the one top button up on your fleece gets all the wenches dripping\\n\", 8360)\n",
      "('whyyy', 'christmas trees on october whyyy???\\n', 8550)\n",
      "('business cat', \"well excuse me, team sarcastic you'd have typos too if you had no thumbs\\n\", 8740)\n",
      "('correction guy', \"get this right! people don't conversate  they converse!\\n\", 8930)\n",
      "('south park aand its gone', \"full battery on my iphone 5 aaaand it's gone\\n\", 9120)\n",
      "('skeptical african child', 'so your telling me not all water is brown\\n', 9310)\n",
      "('so youre telling me', 'so your telling me they tried stopping kony by liking a video?\\n', 9500)\n",
      "('okay guy', 'send message to girl she goes offline okay \\n', 9690)\n",
      "('san juan cholo', 'bitches be like... all i want for my birthday is my two front teeth.\\n', 9880)\n",
      "('sudden realization ralph', 'mike rowe is the white morgan freeman\\n', 10070)\n",
      "('what if i told you matrix morpheus', 'the bottom text is on the top what if i told you\\n', 10260)\n",
      "('kill yourself guy', 'lil wayne makes great songs? kill yourself\\n', 10450)\n",
      "('imagination', 'you saw nothing\\n', 10640)\n",
      "('crying peter parker', 'need to pee cannot pause multiplayer\\n', 10830)\n",
      "('crying peter parker', \"can't you see, i love. you sophie!!!\\n\", 11020)\n",
      "('skeptical african kid', 'you think my heads big.. my penis is even bigger\\n', 11210)\n",
      "('southpark bad time meme', \"if you can't tell your guyfriend and your girlfriend apart you're going to have a bad time\\n\", 11400)\n",
      "('ancient alien guy', 'kommen  nur wenn es chicken gibt. schwarze\\n', 11590)\n",
      "('chemistry cat', 'im in ur freezr makin wif teh alchemy\\n', 11780)\n",
      "('chill out lemur', 'i was just getting to it\\n', 11970)\n",
      "('african children dancing', \"it' s your birthday\\n\", 12160)\n",
      "('skeptical black kid', 'well buttercup i guess  you should have chose a different guy to rob with a plastic gun\\n', 12350)\n",
      "('fat chinese kid', 'trickle down economics: \"give me the cake, and *i* will pass out the slices\"\\n', 12540)\n",
      "('see nobody cares', 'troll, we got a troll here! see, nobody cares.\\n', 12730)\n",
      "('angry school boy', 'break my crayons? a boot to the head.\\n', 12920)\n",
      "('captain picard', 'why do leafs fans hate me? i speak the truth!\\n', 13110)\n",
      "('courage wolf', 'there is no \"i\" in team but there is one in w\"i\"n!\\n', 13300)\n",
      "('matrix morpheus', 'what if i told you  not all furries yiff\\n', 13490)\n",
      "('internet husband', \"he's on gaytube again maybe it's my coffee\\n\", 13680)\n",
      "('successful black man', \"damn, i don't know how many kids i got until i see my roster on the first day of school\\n\", 13870)\n",
      "('that would be great', 'if every one would stop saying yolo that would be great\\n', 14060)\n",
      "('so doge', 'wow much curse very swear\\n', 14250)\n",
      "('batman slappp', 'metta world peace is better than carmelo antho.....\\n', 14440)\n",
      "('provincial man', 'ejaculate 4 times no problem\\n', 14630)\n",
      "('sad trooper', 'i have to shit  the zipper is broken\\n', 14820)\n",
      "('challenge accepted 2', 'make diet last longer than five minutes? challenge accepted\\n', 15010)\n",
      "('advice dog', 'lives 7 years at a time older then you in 3\\n', 15200)\n",
      "('i should buy a boat cat', \"lol i can't read\\n\", 15390)\n",
      "('archer', 'can i offer you a drink? how about this expensive prostitute?\\n', 15580)\n",
      "('stoner dog', 'what if, like, god was sleeping and, like, we`re all just his dream\\n', 15770)\n",
      "('not sure if troll', 'not sure if serious or baiting\\n', 15960)\n",
      "('keep calm and', 'shut the fuck up\\n', 16150)\n",
      "('guess who ', \"guess who don't ask nobody for shit\\n\", 16340)\n",
      "('really stoned guy', 'invited for coffe & smoke brings laptop\\n', 16530)\n",
      "('sunny student', 'how the fuck is this legendary tier?\\n', 16720)\n",
      "('bear grylls', 'a wild snorlax appears drink urine. critical hit\\n', 16910)\n",
      "('ihate', 'why is everything i do completely pointless?\\n', 17100)\n",
      "('paranoid parrot', 'write witty facebook status google each word to make sure you spelt them right\\n', 17290)\n",
      "('rage fu', 'little brother got the t.v. remote first! ffuuuuuuuuuuuuuuuuu\\n', 17480)\n",
      "('socially awesome awkward penguin', \"reminisce about your clever joke that made everyone chuckle  earlier sudden realization that you've been had a creepy smile for at least five minutes while alone in a public setting\\n\", 17670)\n",
      "('anxiety cat', 'smell something funny automatically assume its you & get as far from people as possible.\\n', 17860)\n",
      "('bear grylls loneliness', 'just got fired better drink my own piss\\n', 18050)\n",
      "('anti joke chicken', 'how do you confuse your sister  you hit her with a shovel\\n', 18240)\n",
      "('awkward seal', 'when you try to do a silent fart but u fail\\n', 18430)\n",
      "('really high guy', 'dont drink that water its expired\\n', 18620)\n",
      "('crying girl sad', 'when 2 need advil, 3 come out try to put 1 back, 2 fall in\\n', 18810)\n"
     ]
    }
   ],
   "source": [
    "#CREATE EVALUATION SET\n",
    "#SHOULD BE MEMES WITH REPEATED FORMAT \n",
    "for i in range(100):\n",
    "    #print(data_memes[i*190].replace('/data/alpv95/MemeProject/im2txt/memes/',''),data_captions[i*190],i*190)\n",
    "    print(data_meme_names[i*190],data_captions[i*190],i*190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = [0,570,760,1140,2850,3040,4940,28500]\n",
    "eval_captions = ['y u no','i dont always','not sure if','one does not simply','what if i told you','what if','so youre telling me','so then i said']\n",
    "eval_memes = []\n",
    "\n",
    "for idx in eval_examples:\n",
    "    eval_memes.append(data_memes[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385708"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Should also look at removing examples which are similar but not exactly the same\n",
    "\n",
    "c = list(zip(data_memes, data_captions))\n",
    "no_repeats = []\n",
    "# order preserving\n",
    "def idfun(x): return x\n",
    "\n",
    "seen = {}\n",
    "no_repeats = []\n",
    "for item in c:\n",
    "    marker = idfun(item[1])\n",
    "    # in old Python versions:\n",
    "    # if seen.has_key(marker)\n",
    "    # but in new ones:\n",
    "    if marker in seen: continue\n",
    "    seen[marker] = 1\n",
    "    no_repeats.append(item)\n",
    "print(len(data_captions))\n",
    "len(no_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(no_repeats)\n",
    "memes_shuffled, captions_shuffled = zip(*no_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385708\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    word_captions.append(words)\n",
    "print(len(word_captions))\n",
    "#word_captions = list(set(word_captions))\n",
    "#print(len(word_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary.\n",
      "('Total words:', 150280)\n",
      "('Words in vocabulary:', 41153)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Creating vocabulary.\")\n",
    "counter = Counter()\n",
    "for c in word_captions:\n",
    "    counter.update(c)\n",
    "print(\"Total words:\", len(counter))\n",
    "\n",
    "# Filter uncommon words and sort by descending count.\n",
    "word_counts = [x for x in counter.items() if x[1] >= 3]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Words in vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionary.\n",
    "reverse_vocab = [x[0] for x in word_counts]\n",
    "#unk_id = len(reverse_vocab)\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "38521\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION=300 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = current_dir\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "index_counter = 0\n",
    "\n",
    "with open('glove.42B.300d.txt','r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        if word in vocab_dict:\n",
    "            index_counter += 1\n",
    "            word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "            word2idx[word] = index_counter # PAD is our zeroth index so shift by one\n",
    "            weights.append(word_weights)\n",
    "        if index % 100000 == 0:\n",
    "            print(index)\n",
    "        if index + 1 == 2000000:\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random #maybe include start and end token here\n",
    "UNKNOWN_TOKEN=len(weights)\n",
    "#include * token as its very common? why not\n",
    "word2idx['*'] = UNKNOWN_TOKEN\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN + 1\n",
    "word2idx['<S>'] = UNKNOWN_TOKEN + 2\n",
    "word2idx['</S>'] = UNKNOWN_TOKEN + 3\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "#Save Vocabulary IF NEW\n",
    "#with tf.gfile.FastGFile('vocab4.txt', \"w\") as f:\n",
    "    #f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word2idx.iteritems()]))\n",
    "#print(\"Wrote vocabulary file:\", 'vocab4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[37984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE EMBEDDING MATRIX IF NEW\n",
    "np.savetxt('embedding_matrix5',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_caption.append(word2idx['</S>'])\n",
    "    token_captions.append(token_caption)\n",
    "    \n",
    "#for eval set\n",
    "eval_tokens = []\n",
    "for capt in eval_captions:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    #token_caption.append(word2idx['</S>'])\n",
    "    eval_tokens.append(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38519, 4458, 5638, 1, 193, 815, 27, 286, 3591, 35, 33, 33, 38520]\n",
      "omg tyler, why u have small balls?!!\n",
      "\n",
      "[38519, 46, 137, 31, 716]\n",
      "one does not simply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38518"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_captions[330000])\n",
    "print(captions_shuffled[330000])\n",
    "print(eval_tokens[3])\n",
    "print(eval_captions[3])\n",
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32910\n",
      "42909\n",
      "77716\n",
      "122267\n",
      "123385\n",
      "128041\n",
      "136373\n",
      "160317\n",
      "211864\n",
      "244446\n",
      "256839\n",
      "341853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,ting in enumerate(deleters):\n",
    "    print(ting)\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)\n",
    "len(deleters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30259\n"
     ]
    }
   ],
   "source": [
    "many_unk = []\n",
    "for i,capt in enumerate(token_captions):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        many_unk.append(i)\n",
    "print(len(many_unk))\n",
    "\n",
    "for i,ting in enumerate(many_unk):\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355437"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memes_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "def _bytes_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n",
    "\n",
    "def _floats_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9168698310852050                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  2523229837417602                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0   308641821146011\n",
      "                 0                 0                 0  9107991218566894\n",
      "                 0                 0  6078919887542725                 0\n",
      "                 0                 0                 0                 0\n",
      "  3697678327560425                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0  3752865076065063                 0                 0\n",
      "                 0                 0  4150147914886474  3943297624588012\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0 18090923309326172\n",
      "                 0                 0                 0   659005880355835\n",
      "  3415115118026733                 0                 0                 0\n",
      " 25201379776000976    58916352689266                 0                 0\n",
      "                 0                 0                 0  2246412992477417\n",
      "                 0                 0  6119893550872803  1267738580703735\n",
      "                 0                 0                 0                 0\n",
      "                 0  1782684445381164                 0  5975249767303467\n",
      "                 0  2116162538528442                 0                 0\n",
      "  1107490420341491 12133914947509766                 0                 0]\n"
     ]
    }
   ],
   "source": [
    "#ONLY FOR ALEXNET \n",
    "memes_shuffled_int = []\n",
    "for i,meme in enumerate(memes_shuffled):\n",
    "    memes_shuffled_int.append(np.int_(meme*1000000000000000))\n",
    "print(memes_shuffled_int[20][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR ALEXNET \n",
    "eval_memes_int = []\n",
    "for i,meme in enumerate(eval_memes):\n",
    "    eval_memes_int.append(np.int_(meme*1000000000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR INCEPTION\n",
    "class ImageDecoder(object):\n",
    "    \"\"\"Helper class for decoding images in TensorFlow.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a single TensorFlow Session for all image decoding calls.\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "        # TensorFlow ops for JPEG decoding.\n",
    "        self._encoded_jpeg = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._encoded_jpeg, channels=3)\n",
    "\n",
    "    def decode_jpeg(self, encoded_jpeg):\n",
    "        image = self._sess.run(self._decode_jpeg,\n",
    "                               feed_dict={self._encoded_jpeg: encoded_jpeg})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/355437\n",
      "Train data: 20000/355437\n",
      "Train data: 40000/355437\n",
      "Train data: 60000/355437\n",
      "Train data: 80000/355437\n",
      "Train data: 100000/355437\n",
      "Train data: 120000/355437\n",
      "Train data: 140000/355437\n",
      "Train data: 160000/355437\n",
      "Train data: 180000/355437\n",
      "Train data: 200000/355437\n",
      "Train data: 220000/355437\n",
      "Train data: 240000/355437\n",
      "Train data: 260000/355437\n",
      "Train data: 280000/355437\n",
      "Train data: 300000/355437\n",
      "Train data: 320000/355437\n",
      "Train data: 340000/355437\n"
     ]
    }
   ],
   "source": [
    "#SAVING TRAINING SET (ALEXNET)\n",
    "import sys\n",
    "train_filename = 'train.tfrecordsALEX'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(memes_shuffled_int[i].tostring()), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/355437\n",
      "Train data: 20000/355437\n",
      "Train data: 40000/355437\n",
      "Train data: 60000/355437\n",
      "Train data: 80000/355437\n",
      "Train data: 100000/355437\n",
      "Train data: 120000/355437\n",
      "Train data: 140000/355437\n",
      "Train data: 160000/355437\n",
      "Train data: 180000/355437\n",
      "Train data: 200000/355437\n",
      "Train data: 220000/355437\n",
      "Train data: 240000/355437\n",
      "Train data: 260000/355437\n",
      "Train data: 280000/355437\n",
      "Train data: 300000/355437\n",
      "Train data: 320000/355437\n",
      "Train data: 340000/355437\n"
     ]
    }
   ],
   "source": [
    "#SAVING TRAINING SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'train.tfrecords7'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(memes_shuffled[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/8\n"
     ]
    }
   ],
   "source": [
    "#SAVING EVAL SET (ALEXNET)\n",
    "import sys\n",
    "train_filename = 'eval.tfrecordsALEX'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(eval_memes_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(eval_memes_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(eval_memes_int[i].tostring()), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(eval_tokens[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/8\n"
     ]
    }
   ],
   "source": [
    "#SAVING EVAL SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'eval.tfrecords7'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(eval_memes)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(eval_memes[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(eval_memes))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(eval_tokens[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/alpv95/MemeProject/im2txt/memes/fuckmeright.jpg\n",
      "i don't like cheese on my burgers fuck me, right?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(memes_shuffled[150003])\n",
    "print(captions_shuffled[150003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
