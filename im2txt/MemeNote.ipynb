{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some basic imports and setups\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "\n",
    "#mean of imagenet dataset in BGR\n",
    "imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "image_dir = os.path.join(current_dir, 'memes')\n",
    "#image_dir = current_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "\n",
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000,[],['fc7','fc8'],512,weights_path='/data/alpv95/MemeProject/im2txt/bvlc_alexnet.npy') #maybe need to put fc8 in skip_layers\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc6\n",
    "\n",
    "#create op to calculate softmax \n",
    "#softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Converting captions and meme vector representations into single Tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires putting memes through alexnet to find their vector rep, shuffling the captions, changing captions into their word2idx, finally saving one caption together with one meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194, 194)\n",
      "(100, 19500, 19500, 19500)\n",
      "sizing error\n",
      "(200, 38642, 38642, 38642)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(300, 57715, 57715, 57715)\n",
      "sizing error\n",
      "sizing error\n",
      "(400, 76329, 76329, 76329)\n",
      "sizing error\n",
      "sizing error\n",
      "(500, 94591, 94591, 94591)\n",
      "sizing error\n",
      "sizing error\n",
      "(600, 113146, 113146, 113146)\n",
      "sizing error\n",
      "(700, 131507, 131507, 131507)\n",
      "sizing error\n",
      "(800, 149520, 149520, 149520)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(900, 167518, 167518, 167518)\n",
      "(1000, 185425, 185425, 185425)\n",
      "sizing error\n",
      "(1100, 203294, 203294, 203294)\n",
      "(1200, 220336, 220336, 220336)\n",
      "sizing error\n",
      "(1300, 237823, 237823, 237823)\n",
      "sizing error\n",
      "sizing error\n",
      "(1400, 254129, 254129, 254129)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(1500, 270058, 270058, 270058)\n",
      "(1600, 286622, 286622, 286622)\n",
      "sizing error\n",
      "sizing error\n",
      "(1700, 302200, 302200, 302200)\n",
      "sizing error\n",
      "(1800, 316652, 316652, 316652)\n",
      "sizing error\n",
      "(1900, 331886, 331886, 331886)\n",
      "(2000, 346319, 346319, 346319)\n",
      "sizing error\n",
      "sizing error\n",
      "(2100, 362089, 362089, 362089)\n",
      "(2200, 375126, 375126, 375126)\n",
      "sizing error\n",
      "sizing error\n",
      "(2300, 389123, 389123, 389123)\n",
      "sizing error\n",
      "sizing error\n",
      "(2400, 401660, 401660, 401660)\n",
      "(2500, 414389, 414389, 414389)\n",
      "23\n",
      "414389\n"
     ]
    }
   ],
   "source": [
    "#USE FOR ALEXNET !!!!!!!!!\n",
    "#TRAINING SET\n",
    "\n",
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "data_meme_names = [] #just to check captions have been paired correctly\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "#Doing everything in one script: (the fc6 vectors are quite sparse)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    for i,meme in enumerate(img_files):\n",
    "        #meme_name = meme.replace('/Users/ALP/Desktop/Stanford/CS224n/MemeProject/memes/','')\n",
    "        #meme_name = meme_name.replace('.jpg','').lower()\n",
    "        #meme_name = meme_name.replace('-',' ')\n",
    "        img = Image.open(meme)\n",
    "        try:\n",
    "            img.thumbnail((227, 227), Image.ANTIALIAS)\n",
    "            #img = img.resize((227,227))\n",
    "            #use img.thumbnail for square images, img.resize for non square\n",
    "            assert np.shape(img) == (227, 227, 3)\n",
    "        except AssertionError:\n",
    "            img = img.resize((227,227))\n",
    "            print('sizing error')\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img = img - imagenet_mean #should probably change this\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "\n",
    "        meme_vector = sess.run(score, feed_dict={x: img, keep_prob: 1}) #[1,4096]\n",
    "        meme_vector = np.reshape(meme_vector,[4096])\n",
    "        assert np.shape(meme_vector) == (4096,)\n",
    "        \n",
    "        match = []\n",
    "        meme_name = captions[counter].split(' - ')[0]\n",
    "        image_name = img_files[i].replace('/data/alpv95/MemeProject/im2txt/memes/','')\n",
    "        image_name = image_name.replace('.jpg','')\n",
    "        image_name = image_name.replace('-',' ')\n",
    "        #print(meme_name,image_name)\n",
    "        try:\n",
    "            assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "        except AssertionError:\n",
    "            passed+=1\n",
    "            continue\n",
    "        \n",
    "        while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "            if counter==len(captions)-1:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                break\n",
    "            elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "                counter += 1\n",
    "            else:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                counter += 1\n",
    "                \n",
    "        \n",
    "        #now save in tfrecords format, or prepare for that action\n",
    "        meme_vectors = [meme_vector for cap in match]\n",
    "        image_names = [image_name for cap in match]\n",
    "        assert len(meme_vectors) == len(match)\n",
    "        data_memes.extend(meme_vectors)\n",
    "        data_captions.extend(match)\n",
    "        data_meme_names.extend(image_names)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i,len(data_memes),len(data_captions),len(data_meme_names))\n",
    "\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194, 194)\n",
      "(100, 19500, 19500, 195)\n",
      "(200, 38642, 38642, 186)\n",
      "(300, 57715, 57715, 193)\n",
      "(400, 76329, 76329, 193)\n",
      "(500, 94590, 94590, 195)\n",
      "(600, 113145, 113145, 193)\n",
      "(700, 131506, 131506, 193)\n",
      "(800, 149519, 149519, 188)\n",
      "(900, 167517, 167517, 145)\n",
      "(1000, 185424, 185424, 182)\n",
      "(1100, 203293, 203293, 178)\n",
      "(1200, 220335, 220335, 191)\n",
      "(1300, 237822, 237822, 193)\n",
      "(1400, 254128, 254128, 188)\n",
      "(1500, 270057, 270057, 106)\n",
      "(1600, 286621, 286621, 190)\n",
      "(1700, 302199, 302199, 135)\n",
      "(1800, 316651, 316651, 191)\n",
      "(1900, 331885, 331885, 195)\n",
      "(2000, 346318, 346318, 192)\n",
      "(2100, 362088, 362088, 1)\n",
      "(2200, 375125, 375125, 188)\n",
      "(2300, 389122, 389122, 23)\n",
      "(2400, 401659, 401659, 189)\n",
      "(2500, 414388, 414388, 1)\n",
      "23\n",
      "414388\n"
     ]
    }
   ],
   "source": [
    "#USE FOR INCEPTION NETWORK (OR ALEXNET WITH FINETUNING) #NOW WITH LABELS INCLUDED\n",
    "#TRAINING SET\n",
    "#For this case just need to save image filenames alongside captions\n",
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/CaptionsClean.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "data_labels = []\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "for i,meme in enumerate(img_files):\n",
    "    \n",
    "    match = []\n",
    "    meme_name = captions[counter].split(' - ')[0]\n",
    "    image_name = meme.replace('/data/alpv95/MemeProject/im2txt/memes/','')\n",
    "    image_name = image_name.replace('.jpg','')\n",
    "    image_name = image_name.replace('-',' ')\n",
    "    #print(meme_name,image_name)\n",
    "    try:\n",
    "        assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "    except AssertionError:\n",
    "        passed+=1\n",
    "        continue\n",
    "        \n",
    "    while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "        if counter==len(captions)-1:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            break\n",
    "        elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "            counter += 1\n",
    "        else:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            counter += 1\n",
    "                \n",
    "        \n",
    "    #now save in tfrecords format, or prepare for that action\n",
    "    meme_images = [meme for cap in match]\n",
    "    meme_labels = [meme_name for cap in match]\n",
    "    assert len(meme_images) == len(match)\n",
    "    data_memes.extend(meme_images)\n",
    "    data_captions.extend(match)\n",
    "    data_labels.extend(meme_labels)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i,len(data_memes),len(data_captions),len(meme_labels))\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/alpv95/MemeProject/im2txt/memes/i-hardly-know-her.jpg\n",
      "hebrew? i hardly knew you!\n",
      "\n",
      "i hardly know her\n",
      "/data/alpv95/MemeProject/im2txt/memes/typical-grandmother.jpg\n",
      "suor grazia\n",
      "\n",
      "typical grandmother\n",
      "/data/alpv95/MemeProject/im2txt/memes/shabbywag.jpg\n",
      "fuckshitbitch pissassdick\n",
      "\n",
      "shabbywag\n",
      "/data/alpv95/MemeProject/im2txt/memes/facebook-roleplay-ocelot.jpg\n",
      "i not understand your speking inglish rite?\n",
      "\n",
      "facebook roleplay ocelot\n",
      "/data/alpv95/MemeProject/im2txt/memes/facebook-roleplay-ocelot.jpg\n",
      "happy new year alex!! miss & love ya bunches!! <3\n",
      "\n",
      "facebook roleplay ocelot\n",
      "/data/alpv95/MemeProject/im2txt/memes/funny-stupid.jpg\n",
      "and make good choices\n",
      "\n",
      "funny stupid\n",
      "/data/alpv95/MemeProject/im2txt/memes/unlucky-person.jpg\n",
      "egd idag och imorgon\n",
      "\n",
      "unlucky person\n",
      "/data/alpv95/MemeProject/im2txt/memes/indian-gangster-wannabe.jpg\n",
      "my name is anoop aka anoop dogg\n",
      "\n",
      "indian gangster wannabe\n",
      "/data/alpv95/MemeProject/im2txt/memes/indian-gangster-wannabe.jpg\n",
      "gua miss sama lu ah anak ayamkuuu\n",
      "\n",
      "indian gangster wannabe\n",
      "/data/alpv95/MemeProject/im2txt/memes/happy-golfer.jpg\n",
      "trusted instincts always wins\n",
      "\n",
      "happy golfer\n",
      "/data/alpv95/MemeProject/im2txt/memes/happy-golfer.jpg\n",
      "snow day yes!!!!!!\n",
      "\n",
      "happy golfer\n",
      "/data/alpv95/MemeProject/im2txt/memes/lamenting-lemur.jpg\n",
      "dont eat your cake unless you want to poop it all out\n",
      "\n",
      "lamenting lemur\n",
      "/data/alpv95/MemeProject/im2txt/memes/lamenting-lemur.jpg\n",
      "i think i would be cooler... if i shared the same birthday as jon\n",
      "\n",
      "lamenting lemur\n",
      "/data/alpv95/MemeProject/im2txt/memes/epic-beard-man.jpg\n",
      "wtf dude\n",
      "\n",
      "epic beard man\n",
      "/data/alpv95/MemeProject/im2txt/memes/epic-beard-man.jpg\n",
      "beards... cause fuck chin's\n",
      "\n",
      "epic beard man\n",
      "/data/alpv95/MemeProject/im2txt/memes/2006scape.jpg\n",
      "icecream is offisive lock all threads!\n",
      "\n",
      "2006scape!\n",
      "/data/alpv95/MemeProject/im2txt/memes/2006scape.jpg\n",
      "july 2013... our busiest month yet..\n",
      "\n",
      "2006scape!\n",
      "/data/alpv95/MemeProject/im2txt/memes/cryingbreivik.jpg\n",
      "your telling me jocelyn hasn't shaved bitch is getting a razor for christmas\n",
      "\n",
      "cryingbreivik\n",
      "/data/alpv95/MemeProject/im2txt/memes/misunderstood-russia.jpg\n",
      "bludgeons you over the head with a pillow during the pillow fight\n",
      "\n",
      "misunderstood russia\n",
      "/data/alpv95/MemeProject/im2txt/memes/misunderstood-russia.jpg\n",
      "i will beat you  in a game of tetris\n",
      "\n",
      "misunderstood russia\n",
      "/data/alpv95/MemeProject/im2txt/memes/lime-guy.jpg\n",
      "why can't i, hold all these grudges\n",
      "\n",
      "lime guy\n",
      "/data/alpv95/MemeProject/im2txt/memes/lime-guy.jpg\n",
      "why can't i hold all opening these packs\n",
      "\n",
      "lime guy\n",
      "/data/alpv95/MemeProject/im2txt/memes/cynical-animeshniki.jpg\n",
      "language of music extension? not f*cking likely\n",
      "\n",
      "cynical animeshniki\n",
      "/data/alpv95/MemeProject/im2txt/memes/whipped-boyfriend-perry.jpg\n",
      "gets his band's single to number 1 on the charts \"pheobe\" by \"pheobe\"\n",
      "\n",
      "whipped boyfriend perry\n",
      "/data/alpv95/MemeProject/im2txt/memes/whipped-boyfriend-perry.jpg\n",
      "my mates are like cream whipped!\n",
      "\n",
      "whipped boyfriend perry\n",
      "/data/alpv95/MemeProject/im2txt/memes/hannibalbarca13.jpg\n",
      "i hate capitalism and eating less than 8000 calories per meal\n",
      "\n",
      "hannibalbarca13\n",
      "/data/alpv95/MemeProject/im2txt/memes/hannibalbarca13.jpg\n",
      "oh yeah? costco huh?  cool story bro\n",
      "\n",
      "hannibalbarca13\n",
      "/data/alpv95/MemeProject/im2txt/memes/bandwagon-baboon.jpg\n",
      "leigh is a babbling baboon\n",
      "\n",
      "bandwagon baboon\n",
      "/data/alpv95/MemeProject/im2txt/memes/bandwagon-baboon.jpg\n",
      "baboons breaking through windows south african problems\n",
      "\n",
      "bandwagon baboon\n",
      "/data/alpv95/MemeProject/im2txt/memes/sumoface.jpg\n",
      "i accidently put yeast  in my head when i was little now look at me\n",
      "\n",
      "sumoface\n",
      "/data/alpv95/MemeProject/im2txt/memes/vesyolyi-pochvoved.jpg\n",
      "after you get a murdoch degree\n",
      "\n",
      "vesyolyi-pochvoved\n",
      "/data/alpv95/MemeProject/im2txt/memes/raging-metal-chick.jpg\n",
      "blommewienkel was wegens omstandigheden gesloten\n",
      "\n",
      "raging metal chick\n",
      "/data/alpv95/MemeProject/im2txt/memes/maple-noob.jpg\n",
      "miky  the house of the mouse\n",
      "\n",
      "maple noob\n",
      "/data/alpv95/MemeProject/im2txt/memes/bad-luck-cyhi.jpg\n",
      "good is the kush      ymcmb is the mid-grade cyhi is the snicklefritz\n",
      "\n",
      "bad luck cyhi\n",
      "/data/alpv95/MemeProject/im2txt/memes/bad-luck-cyhi.jpg\n",
      "cs leaked? pm please\n",
      "\n",
      "bad luck cyhi\n",
      "/data/alpv95/MemeProject/im2txt/memes/misophonia-meerkat.jpg\n",
      "pizza cows\n",
      "\n",
      "misophonia meerkat\n",
      "/data/alpv95/MemeProject/im2txt/memes/misophonia-meerkat.jpg\n",
      "\"just ignore it\" just stop it\n",
      "\n",
      "misophonia meerkat\n",
      "/data/alpv95/MemeProject/im2txt/memes/vk-voditel.jpg\n",
      "mi carrito es ferrari \n",
      "\n",
      "vk_voditel\n",
      "/data/alpv95/MemeProject/im2txt/memes/pans-labyrinth1.jpg\n",
      "oh, you're afraid the weather will ruin your hair? let me get a closer look.  \n",
      "\n",
      "pan's labyrinth1\n",
      "/data/alpv95/MemeProject/im2txt/memes/pans-labyrinth1.jpg\n",
      "i'll have the sandusky-tini. \n",
      "\n",
      "pan's labyrinth1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,50):\n",
    "    print(data_memes[i*100+402000])\n",
    "    print(data_captions[i*100+402000])\n",
    "    print(data_labels[i*100+402000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arashian alpaca'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels[len(data_memes)-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('successful-mexican.jpg', 'tank jou for calling dell supprt press 1 for spanish, 2 for mexican, 3 for columbian, 4 for mayan, 5 for aztec, or just hang up.\\n', 38000)\n",
      "('office-space-boss.jpg', 'if you could let me eat my burrito in peace that would be great\\n', 38190)\n",
      "('calamardo-me-vale.jpg', 'viva!!! finalmente a anac liberou meu cht!\\n', 38380)\n",
      "('professor-oak.jpg', \"i've spent my entire life researching pokemon take this pokedex. it's empty\\n\", 38570)\n",
      "('bad-factman.jpg', 'not owlman i m batman\\n', 38760)\n",
      "('bender-imho.jpg', \"i'll have my own youth councils with blackjack and hookers\\n\", 38950)\n",
      "('angry-linus.jpg', 'hey windows, linux runs on the most server, so fuck you!\\n', 39140)\n",
      "('stupid-monkey.jpg', '. thats the point\\n', 39330)\n",
      "('unimpressed-queen.jpg', 'djibouti? not amused\\n', 39520)\n",
      "('you-mean-to-tell-me-black-kid.jpg', \"so you're telling me that to be a seahawks fan, all i have to do is wait for a winning season, buy a jersey, and talk trash?\\n\", 39710)\n",
      "('ryan-gosling-hey-girl-3.jpg', \"hey girl that sweater will look good on you, because you're wearing it. also. grammar is sexy ;)\\n\", 39900)\n",
      "('mckayla-maroney-not-impressed.jpg', 'chupalo\\n', 40090)\n",
      "('i-will-find-you-meme.jpg', 'suck my sock so hard that i pop out of my fucking shell! or i will find you, and i will kill you....\\n', 40280)\n",
      "('i-double-dare-you.jpg', 'i dare you to call me a little girl again i double dare you motherfucker\\n', 40470)\n",
      "('metal-boy-from-hell.jpg', 'buys 12 string guitar... djents\\n', 40660)\n",
      "('god.jpg', \"my book says that only fools believe i don't exist i am the one who gave people brains with very varying intelligence\\n\", 40850)\n",
      "('obama-beer.jpg', \"open on president's day? not bad\\n\", 41040)\n",
      "('confused-bill-cosby.jpg', 'the same time an unarmed young man gets killed  an ice bucket challenge goes viral\\n', 41230)\n",
      "('minecraft-guy.jpg', 'spending hours mining for ideas but always falling into lava\\n', 41420)\n",
      "('i-dont-always-guy-meme.jpg', 'we dont always hit 3.0 as a team but when we do, its to gain access to the keg\\n', 41610)\n",
      "('buddy-the-elf.jpg', 'even i would be a better joker look at the smile\\n', 41800)\n",
      "('dr-evil-and-his-minions.jpg', \"we'll hire employees from all ethnic backrounds  then we'll make the one you can't understand the cashier\\n\", 41990)\n",
      "('highdrunk-guy.jpg', 'jual motor  buat modif motor \\n', 42180)\n",
      "('so-i-got-that-going-on-for-me-which-is-nice.jpg', \"the feds haven't kicked in my door yet so i got that going for me which is nice\\n\", 42370)\n",
      "('liberal-douche-garofalo.jpg', 'promotes healthy living. is a drug-addict.\\n', 42560)\n",
      "('everywhere.jpg', 'publishing    publishing everywhere\\n', 42750)\n",
      "('angry-cat-meme.jpg', 'hey girls did you know that no\\n', 42940)\n",
      "('x-all-the-y.jpg', 'write all the stories!\\n', 43130)\n",
      "('youre-gonna-have-a-bad-time.jpg', \"if you kill joan you're gonna have a bad time\\n\", 43320)\n",
      "('mens-wearhouse.jpg', \"we're gonna have a good month i guarantee it\\n\", 43510)\n",
      "('socially-awesome-penguin.jpg', 'tagged in a facebook picture looks great!\\n', 43700)\n",
      "('jealous-potteroman.jpg', 'yeah i dated hedwig  try and prove me wrong\\n', 43890)\n",
      "('kill-yourself-guy-blank.jpg', '*of\\n', 44080)\n",
      "('nick-cage.jpg', \"well that's  cool!\\n\", 44270)\n",
      "('tobey-maguire.jpg', 'popcorn walks by: your face\\n', 44460)\n",
      "('overly-attached-girl.jpg', 'how do i want my eggs? fertilized\\n', 44650)\n",
      "('bill-murray-caddyshack.jpg', 'damn skippy!\\n', 44840)\n",
      "('douchebag-roommate.jpg', 'clogs toilet refuses to plunge\\n', 45030)\n",
      "('fallout-3.jpg', 'sees deathclaw uses mines to cripple legs\\n', 45220)\n",
      "('kd-you-the-real-mvp-f.jpg', 'oberlin class of 2014 you the real mvps\\n', 45410)\n",
      "('chronic-illness-cat.jpg', 'really crappy day period starts just to add to my misery\\n', 45600)\n",
      "('stalin-says.jpg', 'you think you won kgb\\n', 45790)\n",
      "('correction-man.jpg', 'sad\\n', 45980)\n",
      "('why-cant-i-hold-all-these.jpg', \"why can't i, hold all these gifts?\\n\", 46170)\n",
      "('steve-jobs-says.jpg', \"my life didn't flash before my eyes apple doesn't support flash\\n\", 46360)\n",
      "('socially-awkward-to-awesome-penguin.jpg', \"running in daylight with a reflective vest because you're running past sunset\\n\", 46550)\n",
      "('foul-bachelorette-frog.jpg', 'get out of shower period blood runs down legs. get back in shower.\\n', 46740)\n",
      "('asinine-america.jpg', 'the true  american\\n', 46930)\n",
      "('beavis-and-butthead.jpg', 'highest comment totals posted in 2012 on yahoo! news articles\\n', 47120)\n",
      "('justin-bieber.jpg', 'is supposedly 18  still looks 12\\n', 47310)\n",
      "('the-godfather.jpg', 'who do you want taken care off?\\n', 47500)\n",
      "('carl-spackler.jpg', \"single again, but i'll save money on valentine's day so i got that goin' for me which is nice\\n\", 47690)\n",
      "('black-kid.jpg', 'jean has birthday 12.1 whole family goes wild\\n', 47880)\n",
      "('nerdy-kid-lolz.jpg', \"goes to the superbowl what's this cord for?\\n\", 48070)\n",
      "('you-should-feel-bad.jpg', 'your trumpet memes are bad and you should feel bad\\n', 48260)\n",
      "('batman-bitchslap.jpg', 'like if.... no!\\n', 48450)\n",
      "('un-dia-con-paoly.jpg', 'mas corto que temporada de the walking dead wena!\\n', 48640)\n",
      "('chuck-norris-advice.jpg', 'i can count to infinity\\n', 48830)\n",
      "('aaaand-its-gone.jpg', 'zack finally gets here for dota aaaand hes gone\\n', 49020)\n",
      "('teacher.jpg', '\"i have a phd\" doesn\\'t know how to teach\\n', 49210)\n",
      "('scumbag-god.jpg', 'says \"no premarital sex\" makes marital sex boring\\n', 49400)\n",
      "('spock.jpg', 'spock smashes scissors i win\\n', 49590)\n",
      "('dr-evil-quotation-marks.jpg', 'mad skillz\\n', 49780)\n",
      "('pawn-stars-rick.jpg', 'change for a dollar? 75 cents is the best i can do. \\n', 49970)\n",
      "('laughing-girls.jpg', 'i just told the party i had anal sex\\n', 50160)\n",
      "('yes-this-is-dog.jpg', 'no ian, you hang up no, you hang up\\n', 50350)\n",
      "('aboriginal.jpg', 'blacks have  a culture of child abuse and neglect\\n', 50540)\n",
      "('nfl-ref-meeting.jpg', 'after further review we must review further\\n', 50730)\n",
      "('dancing-black-kid.jpg', 'fuck hideout come to africa marvin\\n', 50920)\n",
      "('a-mi-no-me.jpg', 'mmmmmmmm no!\\n', 51110)\n",
      "('age-of-empires.jpg', 'how does one destroy an empire? [torpedo]\\n', 51300)\n",
      "('katt-williams-shocked.jpg', \"syria shouldn't have been talkin shit!\\n\", 51490)\n",
      "('never-have-i-been-so-wrong.jpg', 'i thought fighting cazadores was going to be easy never have i been so wrong\\n', 51680)\n",
      "('dolan-duck.jpg', 'dalay pls\\n', 51870)\n",
      "('anchorman-birthday.jpg', 'great story! compelling, and rich...\\n', 52060)\n",
      "('not-today-arya.jpg', 'what to we say when pondering to call tobbe? not today.\\n', 52250)\n",
      "('dogeeeee.jpg', 'much spry wow very tpp\\n', 52440)\n",
      "('you-have-no-power-here.jpg', 'you are not the assistant supervisor you have no power here\\n', 52630)\n",
      "('bad-advice-cat.jpg', 'bored of mw2? buy mw3!\\n', 52820)\n",
      "('i-will-find-you-and-kill-you.jpg', '\"i got balls of steel!\" i dont know who you are, but i will find 3d realms and i will kill them!\\n', 53010)\n",
      "('diagnostic-house.jpg', \"yes it's lupus\\n\", 53200)\n",
      "('patrick-stewart-wtf.jpg', 'why the fuck would you cut the fat off of a ribeye\\n', 53390)\n",
      "('if-you-know-what.jpg', 'always online for a good reason if you know what i mean\\n', 53580)\n",
      "('i-can-haz.jpg', 'i can has dialysis?\\n', 53770)\n",
      "('angry-black-woman.jpg', 'they suggested oranges\\n', 53960)\n",
      "('evil-plan-kid.jpg', 'it would be a shame if the sprinklers came on\\n', 54150)\n",
      "('baby-wag.jpg', 'right on! saving money!!\\n', 54340)\n",
      "('generic-indian-guy.jpg', 'yolmt\\n', 54530)\n",
      "('science-cat.jpg', 'clearly, you are a slut.\\n', 54720)\n",
      "('you-were-the-chosen-one.jpg', 'ash kacthim you are the chosen 1........ or one or won...... wutever\\n', 54910)\n",
      "('storytime-jesus.jpg', ' you will hear of wars and rumors of wars, but see to it that you are not alarmed.\\n', 55100)\n",
      "('pushing-patrick.jpg', \"why don't we take this tour and bring it to america\\n\", 55290)\n",
      "('shirtless-ryan-gosling.jpg', 'hey girl seriously tho...i find the regal appeal of your scarves so sexy\\n', 55480)\n",
      "('burning-house-girl.jpg', 'my curry smells delicious  your incense smelled like shit\\n', 55670)\n",
      "('grumpy-cat-santa-hat.jpg', 'deck the halls with boughs of holly falalalalala shut the fuck up\\n', 55860)\n",
      "('oh-mer-gerd.jpg', 'ermahgerd sermer rehderng\\n', 56050)\n",
      "('dolan-meme.jpg', 'future pls\\n', 56240)\n",
      "('my-precious-gollum.jpg', 'my precious reports!\\n', 56430)\n",
      "('fat-guy.jpg', 'one cannot eat mc donaldas unless he is vishal hirrawat\\n', 56620)\n",
      "('thespian-peacock.jpg', \"is that your prop? i. didn't. think. so.\\n\", 56810)\n"
     ]
    }
   ],
   "source": [
    "#CREATE EVALUATION SET    #NOW WITH LABELS AS WELL\n",
    "#SHOULD BE MEMES WITH REPEATED FORMAT \n",
    "for i in range(200,300):\n",
    "    print(data_memes[i*190].replace('/data/alpv95/MemeProject/im2txt/memes/',''),data_captions[i*190],i*190)\n",
    "    #print(data_meme_names[i*191],data_captions[i*191],i*191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = [0,570,760,1140,2850,3040,4940,28500]\n",
    "eval_captions = ['y u no','i dont always','not sure if','one does not simply','what if i told you','what if','so youre telling me','so then i said']\n",
    "eval_labels = []\n",
    "eval_memes = []\n",
    "\n",
    "for idx in eval_examples:\n",
    "    eval_memes.append(data_memes[idx])\n",
    "    eval_labels.append(data_labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y u no',\n",
       " 'the most interesting man in the world',\n",
       " 'futurama fry',\n",
       " 'one does not simply',\n",
       " 'what if i told you',\n",
       " 'conspiracy keanu',\n",
       " 'skeptical 3rd world kid',\n",
       " 'so then i said...']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385707"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Should also look at removing examples which are similar but not exactly the same ##WITH LABELS\n",
    "\n",
    "c = list(zip(data_memes, data_captions, data_labels))\n",
    "no_repeats = []\n",
    "# order preserving\n",
    "def idfun(x): return x\n",
    "\n",
    "seen = {}\n",
    "no_repeats = []\n",
    "for item in c:\n",
    "    marker = idfun(item[1])\n",
    "    # in old Python versions:\n",
    "    # if seen.has_key(marker)\n",
    "    # but in new ones:\n",
    "    if marker in seen: continue\n",
    "    seen[marker] = 1\n",
    "    no_repeats.append(item)\n",
    "print(len(data_captions))\n",
    "len(no_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(no_repeats)\n",
    "memes_shuffled, captions_shuffled, labels_shuffled = zip(*no_repeats)\n",
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)\n",
    "labels_shuffled = list(labels_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771414\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_captions = []\n",
    "for capt in captions_shuffled + labels_shuffled: #include labels_shuffled here for glove averages\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    word_captions.append(words)\n",
    "print(len(word_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary.\n",
      "('Total words:', 150760)\n",
      "('Words in vocabulary:', 41819)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Creating vocabulary.\")\n",
    "counter = Counter()\n",
    "for c in word_captions:\n",
    "    counter.update(c)\n",
    "print(\"Total words:\", len(counter))\n",
    "\n",
    "# Filter uncommon words and sort by descending count.\n",
    "word_counts = [x for x in counter.items() if x[1] >= 3]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Words in vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionary.\n",
    "reverse_vocab = [x[0] for x in word_counts]\n",
    "#unk_id = len(reverse_vocab)\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[':']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "38821\n",
      "('Wrote vocabulary file:', 'vocab_averages.txt')\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION=300 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = current_dir\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "index_counter = 0\n",
    "\n",
    "with open('glove.42B.300d.txt','r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        if word in vocab_dict:\n",
    "            index_counter += 1\n",
    "            word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "            word2idx[word] = index_counter # PAD is our zeroth index so shift by one\n",
    "            weights.append(word_weights)\n",
    "        if index % 100000 == 0:\n",
    "            print(index)\n",
    "        if index + 1 == 2000000:\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.zeros(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random #maybe include start and end token here\n",
    "UNKNOWN_TOKEN=len(weights)\n",
    "#include * token as its very common? why not\n",
    "word2idx['*'] = UNKNOWN_TOKEN\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN + 1\n",
    "word2idx['<S>'] = UNKNOWN_TOKEN + 2\n",
    "word2idx['</S>'] = UNKNOWN_TOKEN + 3\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "#Save Vocabulary IF NEW\n",
    "with tf.gfile.FastGFile('vocab_averages.txt', \"w\") as f:\n",
    "    f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word2idx.iteritems()]))\n",
    "print(\"Wrote vocabulary file:\", 'vocab_averages.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[37984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE EMBEDDING MATRIX IF NEW\n",
    "np.savetxt('embedding_matrix_averages',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_caption.append(word2idx['</S>'])\n",
    "    token_captions.append(token_caption)\n",
    "    \n",
    "#for training labels\n",
    "token_labels = []\n",
    "for capt in labels_shuffled:\n",
    "    token_caption = []\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_labels.append(token_caption)\n",
    "    \n",
    "#for eval set\n",
    "eval_tokens = []\n",
    "for capt in eval_captions:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    #token_caption.append(word2idx['</S>'])\n",
    "    eval_tokens.append(token_caption)\n",
    "    \n",
    "#for eval labels\n",
    "token_labels_eval = []\n",
    "for capt in eval_labels:\n",
    "    token_caption = []\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_labels_eval.append(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38819, 213, 20605, 35878, 189, 38820]\n",
      "another brill-iant game\n",
      "\n",
      "[929, 14415, 15800]\n",
      "james \"terminator\" perch\n",
      "[38819, 47, 138, 32, 717]\n",
      "one does not simply\n",
      "[47, 138, 32, 717]\n",
      "one does not simply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38818"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_captions[330000])\n",
    "print(captions_shuffled[330000])\n",
    "print(token_labels[330000])\n",
    "print(labels_shuffled[330000])\n",
    "print(eval_tokens[3])\n",
    "print(eval_captions[3])\n",
    "print(token_labels_eval[3])\n",
    "print(eval_labels[3])\n",
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56483\n",
      "107158\n",
      "149218\n",
      "163116\n",
      "182546\n",
      "212836\n",
      "216460\n",
      "294672\n",
      "312782\n",
      "325379\n",
      "359315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,ting in enumerate(deleters):\n",
    "    print(ting)\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "    del labels_shuffled[ting-i]\n",
    "    del token_labels[ting-i]\n",
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)\n",
    "len(deleters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30209\n"
     ]
    }
   ],
   "source": [
    "many_unk = []\n",
    "for i,capt in enumerate(token_captions):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        many_unk.append(i)\n",
    "print(len(many_unk))\n",
    "\n",
    "for i,ting in enumerate(many_unk):\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "    del labels_shuffled[ting-i]\n",
    "    del token_labels[ting-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    }
   ],
   "source": [
    "unk_label = []\n",
    "for i,capt in enumerate(token_labels):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        unk_label.append(i)\n",
    "print(len(unk_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bntu\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "vesyolyi-pochvoved\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vesyolyi-pochvoved\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "suleyman-kerimov\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vesyolyi-pochvoved\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vaverka-perakladczyca\n",
      "typacal viewy\n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "vesyolyi-pochvoved\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vesyolyi-pochvoved\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vesyolyi-pochvoved\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "vaverka-perakladczyca\n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vesyolyi-pochvoved\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bgeu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vesyolyi-pochvoved\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "typacal viewy\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vesyolyi-pochvoved\n",
      "assburgerin superneuvo\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "vaverka-perakladczyca\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "assburgerin superneuvo\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "typacal viewy\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "ranzigen peelaert \n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "vesyolyi-pochvoved\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vesyolyi-pochvoved\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgeu\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "orig_unk_labels = []\n",
    "for i in range(len(unk_label)):\n",
    "    print(labels_shuffled[unk_label[i]])\n",
    "    orig_unk_labels.append(labels_shuffled[unk_label[i]])\n",
    "print(len(set(orig_unk_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "def _bytes_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n",
    "\n",
    "def _floats_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9168698310852050                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  2523229837417602                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0   308641821146011\n",
      "                 0                 0                 0  9107991218566894\n",
      "                 0                 0  6078919887542725                 0\n",
      "                 0                 0                 0                 0\n",
      "  3697678327560425                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0  3752865076065063                 0                 0\n",
      "                 0                 0  4150147914886474  3943297624588012\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0 18090923309326172\n",
      "                 0                 0                 0   659005880355835\n",
      "  3415115118026733                 0                 0                 0\n",
      " 25201379776000976    58916352689266                 0                 0\n",
      "                 0                 0                 0  2246412992477417\n",
      "                 0                 0  6119893550872803  1267738580703735\n",
      "                 0                 0                 0                 0\n",
      "                 0  1782684445381164                 0  5975249767303467\n",
      "                 0  2116162538528442                 0                 0\n",
      "  1107490420341491 12133914947509766                 0                 0]\n"
     ]
    }
   ],
   "source": [
    "#ONLY FOR ALEXNET \n",
    "memes_shuffled_int = []\n",
    "for i,meme in enumerate(memes_shuffled):\n",
    "    memes_shuffled_int.append(np.int_(meme*1000000000000000))\n",
    "print(memes_shuffled_int[20][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR ALEXNET \n",
    "eval_memes_int = []\n",
    "for i,meme in enumerate(eval_memes):\n",
    "    eval_memes_int.append(np.int_(meme*1000000000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR INCEPTION\n",
    "class ImageDecoder(object):\n",
    "    \"\"\"Helper class for decoding images in TensorFlow.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a single TensorFlow Session for all image decoding calls.\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "        # TensorFlow ops for JPEG decoding.\n",
    "        self._encoded_jpeg = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._encoded_jpeg, channels=3)\n",
    "\n",
    "    def decode_jpeg(self, encoded_jpeg):\n",
    "        image = self._sess.run(self._decode_jpeg,\n",
    "                               feed_dict={self._encoded_jpeg: encoded_jpeg})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/355437\n",
      "Train data: 20000/355437\n",
      "Train data: 40000/355437\n",
      "Train data: 60000/355437\n",
      "Train data: 80000/355437\n",
      "Train data: 100000/355437\n",
      "Train data: 120000/355437\n",
      "Train data: 140000/355437\n",
      "Train data: 160000/355437\n",
      "Train data: 180000/355437\n",
      "Train data: 200000/355437\n",
      "Train data: 220000/355437\n",
      "Train data: 240000/355437\n",
      "Train data: 260000/355437\n",
      "Train data: 280000/355437\n",
      "Train data: 300000/355437\n",
      "Train data: 320000/355437\n",
      "Train data: 340000/355437\n"
     ]
    }
   ],
   "source": [
    "#SAVING TRAINING SET (ALEXNET)\n",
    "import sys\n",
    "train_filename = 'train.tfrecordsALEX'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(memes_shuffled_int[i].tostring()), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/355487\n",
      "Train data: 20000/355487\n",
      "Train data: 40000/355487\n",
      "Train data: 60000/355487\n",
      "Train data: 80000/355487\n",
      "Train data: 100000/355487\n",
      "Train data: 120000/355487\n",
      "Train data: 140000/355487\n",
      "Train data: 160000/355487\n",
      "Train data: 180000/355487\n",
      "Train data: 200000/355487\n",
      "Train data: 220000/355487\n",
      "Train data: 240000/355487\n",
      "Train data: 260000/355487\n",
      "Train data: 280000/355487\n",
      "Train data: 300000/355487\n",
      "Train data: 320000/355487\n",
      "Train data: 340000/355487\n"
     ]
    }
   ],
   "source": [
    "#SAVING TRAINING SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'train.tfrecordsAVERAGES'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(memes_shuffled[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i]),\n",
    "          \"train/labels\": _int64_feature_list(token_labels[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/8\n"
     ]
    }
   ],
   "source": [
    "#SAVING EVAL SET (ALEXNET)\n",
    "import sys\n",
    "train_filename = 'eval.tfrecordsALEX'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(eval_memes_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(eval_memes_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(eval_memes_int[i].tostring()), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(eval_tokens[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/8\n"
     ]
    }
   ],
   "source": [
    "#SAVING EVAL SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'eval.tfrecordsAVERAGES'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(eval_memes)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(eval_memes[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(eval_memes))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(eval_tokens[i]),\n",
    "           \"train/labels\": _int64_feature_list(token_labels_eval[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/alpv95/MemeProject/im2txt/memes/crazy-girlfriend-praying-mantis.jpg\n",
      "other women post happy birthday wishes on fb wall flame on\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(memes_shuffled[150003])\n",
    "print(captions_shuffled[150003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
