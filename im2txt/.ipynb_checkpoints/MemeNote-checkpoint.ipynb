{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some basic imports and setups\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "\n",
    "#mean of imagenet dataset in BGR\n",
    "imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "image_dir = os.path.join(current_dir, 'memes')\n",
    "#image_dir = current_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "\n",
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000,[],['fc7','fc8'],512,weights_path='/data/alpv95/MemeProject/im2txt/bvlc_alexnet.npy') #maybe need to put fc8 in skip_layers\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc6\n",
    "\n",
    "#create op to calculate softmax \n",
    "#softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Converting captions and meme vector representations into single Tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires putting memes through alexnet to find their vector rep, shuffling the captions, changing captions into their word2idx, finally saving one caption together with one meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE FOR ALEXNET !!!!!!!!!\n",
    "#TRAINING SET\n",
    "\n",
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "#Doing everything in one script: (the fc6 vectors are quite sparse)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    for i,meme in enumerate(img_files):\n",
    "        #meme_name = meme.replace('/Users/ALP/Desktop/Stanford/CS224n/MemeProject/memes/','')\n",
    "        #meme_name = meme_name.replace('.jpg','').lower()\n",
    "        #meme_name = meme_name.replace('-',' ')\n",
    "        img = Image.open(meme)\n",
    "        try:\n",
    "            img.thumbnail((227, 227), Image.ANTIALIAS)\n",
    "            #img = img.resize((227,227))\n",
    "            #use img.thumbnail for square images, img.resize for non square\n",
    "            assert np.shape(img) == (227, 227, 3)\n",
    "        except AssertionError:\n",
    "            img = img.resize((227,227))\n",
    "            print('sizing error')\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img = img - imagenet_mean #should probably change this\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "\n",
    "        meme_vector = sess.run(score, feed_dict={x: img, keep_prob: 1}) #[1,4096]\n",
    "        meme_vector = np.reshape(meme_vector,[4096])\n",
    "        assert np.shape(meme_vector) == (4096,)\n",
    "        #match = [s.split('-',1)[-1].lstrip() for s in captions if meme_name in s]\n",
    "        \n",
    "        match = []\n",
    "        meme_name = captions[counter].split(' - ')[0]\n",
    "        image_name = img_files[i].replace('/data/alpv95/MemeProject/im2txt/memes/','')\n",
    "        image_name = image_name.replace('.jpg','')\n",
    "        image_name = image_name.replace('-',' ')\n",
    "        #print(meme_name,image_name)\n",
    "        try:\n",
    "            assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "        except AssertionError:\n",
    "            passed+=1\n",
    "            continue\n",
    "        \n",
    "        while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "            if counter==len(captions)-1:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                break\n",
    "            elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "                counter += 1\n",
    "            else:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                counter += 1\n",
    "                \n",
    "        \n",
    "        #now save in tfrecords format, or prepare for that action\n",
    "        meme_vectors = [meme_vector for cap in match]\n",
    "        assert len(meme_vectors) == len(match)\n",
    "        data_memes.extend(meme_vectors)\n",
    "        data_captions.extend(match)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i,len(data_memes),len(data_captions))\n",
    "\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194)\n",
      "(100, 19500, 19500)\n",
      "(200, 38642, 38642)\n",
      "(300, 57715, 57715)\n",
      "(400, 76329, 76329)\n",
      "(500, 94591, 94591)\n",
      "(600, 113146, 113146)\n",
      "(700, 131507, 131507)\n",
      "(800, 149520, 149520)\n",
      "(900, 167518, 167518)\n",
      "(1000, 185425, 185425)\n",
      "(1100, 203294, 203294)\n",
      "(1200, 220336, 220336)\n",
      "(1300, 237823, 237823)\n",
      "(1400, 254129, 254129)\n",
      "(1500, 270058, 270058)\n",
      "(1600, 286622, 286622)\n",
      "(1700, 302200, 302200)\n",
      "(1800, 316652, 316652)\n",
      "(1900, 331886, 331886)\n",
      "(2000, 346319, 346319)\n",
      "(2100, 362089, 362089)\n",
      "(2200, 375126, 375126)\n",
      "(2300, 389123, 389123)\n",
      "(2400, 401660, 401660)\n",
      "(2500, 414389, 414389)\n",
      "23\n",
      "414389\n"
     ]
    }
   ],
   "source": [
    "#USE FOR INCEPTION NETWORK (OR ALEXNET WITH FINETUNING)\n",
    "#TRAINING SET\n",
    "#For this case just need to save image filenames alongside captions\n",
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "for i,meme in enumerate(img_files):\n",
    "    \n",
    "    match = []\n",
    "    meme_name = captions[counter].split(' - ')[0]\n",
    "    image_name = meme.replace('/data/alpv95/MemeProject/im2txt/memes/','')\n",
    "    image_name = image_name.replace('.jpg','')\n",
    "    image_name = image_name.replace('-',' ')\n",
    "    #print(meme_name,image_name)\n",
    "    try:\n",
    "        assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "    except AssertionError:\n",
    "        passed+=1\n",
    "        continue\n",
    "        \n",
    "    while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "        if counter==len(captions)-1:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            break\n",
    "        elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "            counter += 1\n",
    "        else:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            counter += 1\n",
    "                \n",
    "        \n",
    "    #now save in tfrecords format, or prepare for that action\n",
    "    meme_images = [meme for cap in match]\n",
    "    assert len(meme_images) == len(match)\n",
    "    data_memes.extend(meme_images)\n",
    "    data_captions.extend(match)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i,len(data_memes),len(data_captions))\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo baka sid b'day party to aapvij padse\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo bka  galaxy ma bdhaj vgr kam na che\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo bka galaxy ma nva juni upload to krta revu\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo bka tanish  tu to 1number no lodo che\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo priyank whatsup free che upload krta revu\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo baka  gunga to kadhava naj.....\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "12 out of ten voices in my head agree! i'm totally messed up!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo arpita gunga nai khavana!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo arpita gunga nai khavana!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo nandu  ravan to jivandeep noj\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo arpita gunga nai khavana!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo arpita gunga nai khavana!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo bakudi good morning\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "hey if u cant ping stop looking at my last seen time !!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "tu meri candy, main tera crush when you're smiling, baby i'm blush\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "dekh larki tu meri candy main tera crush when you're smiling, baby i'm blush\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "i'm your candy your are my crush when you smile baby i get blush...\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "that man always know how to make this girl smile  #circle #family #boss' sca\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "dekh bhai \"pal\" hu, naam hi kaafi hai\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo bakudi akho di tv jovanu ne mom nu kam krwanu exam to avya kre\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "i had nothing to do!!!!!!!!!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "kevu  baka/bakudi..... kevi rayi utrayan. ..??\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "smile!!! why? because i said so\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "afternoon dan let me see those teeth lol\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      " of all the goofy smiles in the world, yours is my favorite\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "of all the goofy smiles in the world yours is my favorite\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      " \n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo bka vaghela 6u..atle atitude  to revano j..je thay a kari leje.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "jo bka vaghelau 6u.. atle atitude to revano j tare j thay e kari le je\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "vaghela deepak taklif to revani\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "vaghela amit vaghela is no 1\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "dekh yaar tu mera candy mein teri crush when u smile...baby i blush...;p\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      " \n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "hi hi\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "banana!! \n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/very-optimistic-smile.jpg\n",
      "thinking of my friday's with walter mmmmmmmm....\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "three! three lockouts!     ah, ah, ah\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "in my butt hole there is never a lockout \n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "what do you mean lockouts aren't good for the sport?\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "wants players to cut salaries by 17% takes a personal pay raise of 117%\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hires ian charity as assistant commissioner  locks players out of gm connected\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i'd tell you what crosby's penis taste like . . . but i haven't found it yet.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i dont always lockout but when i do i prefer nhl lockouts\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "due to hurricane sandy all nhl in the northeast will be canceled. oh wait...nevermindz\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "nhl lockout waiting on rapture\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "donald fehr sounds optimistic about reaching a deal deal's off the table\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "where did the hockey go? oh wait thats right im a dumbass\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i dont know what it is theres just something about your face that makes me want to punch you !\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i am a fucking retard\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "we just want more money.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "fans matter i mean, they don't\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hockey's good money's better\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i can't skate but i can take your money\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i eat the wax from sid's old laces\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "the druids of the sahara need a hockey team\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "fans will come back so let's lock it out, baby\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "negotiating: you do what i say, or else you don't play\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "guys, seriously i'm way behind on my rent\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "the game has grown so much so let's stop playing\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "the game has grown so much so let's stop playing\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "shanahan on speed dial\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "shanahan on my speed dial\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "stop calling me the count\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you hate me? i hate me!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hockey? what's a hockey? oh look money!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "fire bettman\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "fuck you pay me\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i'm gary bioman and i like to ruin shit\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "3.3 billion profit 10 teams losing money\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i'm sure they'd love a team in hamilton but they already like hockey there\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "should get laid off\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "no jupe no weagle no hockey for you\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "look,  i washed for supper\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "winter is coming... without hockey\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "brace yourself winter is coming, without hockey\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "everything i touch dies\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "sir, what do you think of the fans? next question please...\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "bettman we have a hulk\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "we have an army of lawyers we have a hulk\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "bettman: we have an army of lawyers fans: we have a hulk\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "wants league to be profitable nashville predators\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i'm not a bad guy i just don't like hockey\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "loves the cock\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "midget nigga  thats that shit i don't like... bang! bang!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "fuck hockey i'm talkin' 'bout money!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "lockout so hard white people want to kill me\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "league is        growing? lock this  fucker out!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "1. lockout 2. ??????? 3. profit (for the nba)\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "put a gary bettman sticker on my car now i keep getting locked out\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i support lockouts not hockey\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i don't always say \"i love hockey\" but when i do, i lie about it\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "lets ian be the gm of the caps they go 81-1 because a perfect record just isn't fair\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "can you believe they gave me this job and i don't even like hockey  #bettmansucks\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "detroit schedules games with other 5 originals and gets winter classic season cancelled\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "people are enjoying this thread? lockout\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "got a hat trick '94, '04, '12\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hat trick legend '94 '04' '12\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "2012/13 vezina winner 0.00 gaa\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "2012/13 vezina winner 0 goals allowed\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "idgaf about the fans\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i am the definition of  thundercunt\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you had delicious trisect crackers and didn't tell me?!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "brian barnwell is my protege\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "brian barnwell is my protege\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "6 points for a qb touchdown brian barnwell is a terrible commish\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "6 points for a qb touchdown? barnwell is an excellent commish\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "ehrmagerd bettmannnnnn\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i  own slaves.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i love lockouts!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "eeeeeeeee!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "sorry you've been locked out\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "cremate him spread his ashes on a collective barganing agreement\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "cremate him  spread his ashes on a new collective bargaining agreement\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "82 down, 2378 to go tee hee i love lockouts!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i don't always cause lockouts oh wait..yes i do\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i love to cancel games don't you?\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i don't always love lockouts oh wait a second. yes i do!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "season fuck thatshit\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "nhl season  nah fuck that shit\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i'd do me like the nhl\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "was going to sean's pool party but we could'nt come to a agreement\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "nhlpa won't accept my offer? well...they can't come to my birthday party\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "nhlpa won't accept my offer well...they can't come to my birthday party!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i take your recent proposal and raise you another lockout\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i take your proposal and raise you another lockout\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i take your proposal and raise you another lockout\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i take your recent proposal and raise you another lockout\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "its all going to plan. why make billions when we can lose.... billions?\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "our plan is working.  why make billions more when we can lose... billions?\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "50/50 he he he\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "wanted: dead or alive charge: murdering a sport\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "suspends a player for saying \"sloppy seconds\" has fucked fans three times\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "stern is retiring? i though commissioners were supposed to be dictators for life?\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "stern is retiring? i thought commissioners were supposed to be dictators for life!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "sandy preparation checklist: cancel winter classic\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "sandy preparation checklist: cancel winter classic\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "while you were at those concerts i broke into your house and used all your spatulas to scratch my balls\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "today's the day! no winter classic for you!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "they won't cancel the nyc marathon? let me show them how it's done.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "has anyone seen my bicycle!?\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "nhl season? no problem i'll just cancel that \n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "gary bettman sport's biggest asshole\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "nhl lockout  waiting on rapture\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "me and bill just need a couple weeks to finga-pop each'othas assholes\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "see! i told you i'm as stupid as i look\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you want hockey? well go fuck yourself!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i hope you all enjoy watching basketball this year.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "ermahgerd!!! bertmahn!!!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "boo me when  i hand out the stanley cup i'm gonna take away your game\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "nba should hire me back so that i can show them how to cancel a whole season\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hehehehehehehe whats so funny bitch\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "wanted: dead $100,000\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "wow.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you wanna dispute the season? try me. i've got alex and adam behind me.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "bettman: wanted dead or... ...well preferably dead\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "my favorite commissioner\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "second best commish right behind towny\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i'm standin at the podium tryna watch my sodium\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "y u no leave me alone red dot? johnny loads a cartridge in the sniper rifle\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "gary bettman ain't nobody got time for him\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "no\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "has more money than god you can still kiss his ass\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hi kiss my ass north america!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "we are close to a deal! just kidding, suck my dick chumps\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you guys are the best fanes ever!!! i hope you find a sport to watch someday.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you guys are the best fans ever!!! i hope you find a sport to watch someday\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i am a  piece of shit\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "$25 fine for littering i smash bettman's brains all over the pavement with my bare hands\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hockey? what's a hockey? ooooh look, money!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "curling now that's a man's sport\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i dont know what it is theres something about your face...\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "150 free punches\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "y u no wipe my cum off your face brian burke? get down on your knees and learn to swallow next time\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "what's all the fuss about? i don't even like hockey\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "su-su sudio\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you want hockey you say?\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "oh, you're a hockey fan? well tough shit\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "this is the face of a guy that loves soccer\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "stop bitching about the lockout read \"fraternal bonds\"\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "lockout? let's teabag this troll!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "they hate me they really hate me\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "put gary bettman sticker on my car now i'm always getting locked out\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "a deal is imminent lol jk better luck next year\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "announces new 10 year cba with 8 year mutual cancellation subsequently cancels 2023-2024 nhl season with option to cancel 2021-2022\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "alright i got what i want still wont ratify he cba \n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "president of the gary fan club\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "i just wan to say sorry to the fans that i am a money grubbing asshole\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "make it up to us gary!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "free center ice you owe it to us gary!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "go back to high school\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "lockout with ur cock out\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "you're a ginger? you get everything late.\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      " fuck the nhl i want more money\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "wants  to add more playoff spots maple leafs fan\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hockey in arizona it'll revolutionize the game!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hockey in arizona it'll revolutionize the game!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "a census once tried to test me so i moved his team to phoenix\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hockey in the south it can't miss!\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "when i'm done this gig i'll run the dr pick'em pool\n",
      "\n",
      "/data/alpv95/MemeProject/im2txt/memes/gary-bettman.jpg\n",
      "hey buchsy stfu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    print(data_memes[i+402000])\n",
    "    print(data_captions[i+402000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_memes[len(data_memes)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('maury-lie-detector.jpg', 'you said you were going to pay the money back  the lie detector shows that was a lie\\n', 19000)\n",
      "('sparta.jpg', \"my shoes don't fit\\n\", 19190)\n",
      "('musically-oblivious-8th-grader.jpg', 'skrillex are cool\\n', 19380)\n",
      "('african-kids-dancing.jpg', 'yayy brother nash to lakers finally soneone to pass the ball\\n', 19570)\n",
      "('unpopular-opinion-puffin.jpg', \"i don't care about ferguson and if it were a black cop and a white boy nobody else would care either\\n\", 19760)\n",
      "('unlucky-brian-strikes-again.jpg', 'uses public bathroom sucked off in glory hole\\n', 19950)\n",
      "('kermit-the-frog.jpg', \" but that's none of my business \\n\", 20140)\n",
      "('y-u-so.jpg', 'justin bieber y u so gay?!\\n', 20330)\n",
      "('1889-10-guy.jpg', \"back in my day, we didn't have children with adhd we had lazy ass kids who needed a good beating\\n\", 20520)\n",
      "('and-its-gone.jpg', 'henderson scores! aaaaaand its 2-2\\n', 20710)\n",
      "('fat-girl.jpg', \"i'd do anything to lose weight besides exercise and eating healthy\\n\", 20900)\n",
      "('evil-toddler-kid2.jpg', \"learned my first word now they'll obey more efficiently\\n\", 21090)\n",
      "('advice-hitler.jpg', 'kills non-aryans not even aryan\\n', 21280)\n",
      "('-sudden-clarity-clarence.jpg', 'people say \"cool down\" or \"heated up\" because cool air lowers and heat rises\\n', 21470)\n",
      "('spongebob-rainbow.jpg', 'rakija\\n', 21660)\n",
      "('american-pride-eagle.jpg', \"we had 50 states puerto rico, you're next\\n\", 21850)\n",
      "('rich-men-laughing.jpg', \"sure, we'll have revenue shares\\n\", 22040)\n",
      "('old-lady.jpg', 'where do you put the stamp on an email?\\n', 22230)\n",
      "('ash-pedreiro.jpg', 'hey gurl lemme squrtle on dem jigglypuffs\\n', 22420)\n",
      "('tough-spongebob.jpg', 'i\\'ll have you know, i played \"pomp and circumstance\" for 8 and a half minutes and only cried from lip pain for 3 of them\\n', 22610)\n",
      "('yeah-if-you-could-just.jpg', \"yeah, if you could stop wasting both of our times that'd be great! \\n\", 22800)\n",
      "('first-day-on-the-internet-kid.jpg', 'spellcheck? will never make a spelling mistake again.\\n', 22990)\n",
      "('ridiculously-photogenic-metalhead.jpg', 'books regular airplane ticket gets ed force one\\n', 23180)\n",
      "('vengeance-dad.jpg', 'she said she was too tired for sex so i put her to sleep\\n', 23370)\n",
      "('pawn-stars.jpg', 'i know why you watch this show waiting for me to have a heart attack\\n', 23560)\n",
      "('oprah-you-get-a.jpg', 'you get ebola! and you get ebola!!! you all get ebola!\\n', 23750)\n",
      "('bender-blackjack-and-hookers.jpg', \"i'll start my own thread with blackjack and hookers\\n\", 23940)\n",
      "('overly-obsessed-girlfriend.jpg', 'sorry today i called you only 13 times i was very busy\\n', 24130)\n",
      "('sassy-black-woman.jpg', 'i voted foe his ass  but he better not try an tax mah ass\\n', 24320)\n",
      "('aand-its-gone.jpg', \"my mom bought nutella aaaand it's gone\\n\", 24510)\n",
      "('gangnam-style.jpg', 'stop farting\\n', 24700)\n",
      "('angry-arnold.jpg', \"9:05 class canceled doesn't send email but posts note on door\\n\", 24890)\n",
      "('hipster-ariel.jpg', 'i know legs are mainstream i asked for them ironically\\n', 25080)\n",
      "('skeptical-third-world-kid.jpg', \"so you'r telling me,you can't eat that food if it touches the ground for more than five seconds?\\n\", 25270)\n",
      "('minecraft-creeper.jpg', 'full inventory? let me help\\n', 25460)\n",
      "('grumpy-cat-good.jpg', 'you like games? you should try russian roulette.\\n', 25650)\n",
      "('retail-robin.jpg', \"i didn't think 80% of america was retarded until i got a job\\n\", 25840)\n",
      "('retail-robin.jpg', 'oh look, someone left a toxic cleaning solvent on the same shelf as the toys. way to go, dickhead.\\n', 26030)\n",
      "('dont-you-squidward.jpg', 'when youre in class and someone farts\\n', 26220)\n",
      "('obama.jpg', \"50% of people now approve of the job i am doing. that's the same 50% who are getting free shit from my government.\\n\", 26410)\n",
      "('why-the-fuck.jpg', \"why the fuck are people downvoting snoop's ama?\\n\", 26600)\n",
      "('gordo-granudo.jpg', 'he hit me! thankfully i can restore my hp on the pokecenter\\n', 26790)\n",
      "('newspaper-cat-realization.jpg', 'i should learn how to makeup\\n', 26980)\n",
      "('art-student-owl.jpg', 'life goal: toil in obscurity and die young\\n', 27170)\n",
      "('mean-girls.jpg', 'stop trying to make smeared happen its not going to happen\\n', 27360)\n",
      "('hipster-kitty.jpg', 'i like skrillex because hating him is too mainstream\\n', 27550)\n",
      "('so-i-guess-you-could-say-things-are-getting-pretty-serious.jpg', 'she sends me smileys sometimes so i guess you could say things are getting pretty serious\\n', 27740)\n",
      "('craig-would-be-so-happy.jpg', 'if i could take a shower without the water temperature drastically changing every 30 seconds i would be so happy\\n', 27930)\n",
      "('i-dont-know-who-you-are.jpg', \"i dont know who you are 'bad luck brian' but i traced your call and i will be there in 4 minutes\\n\", 28120)\n",
      "('buddy-jesus.jpg', \"oh no he didn't!\\n\", 28310)\n",
      "('so-then-i-said.jpg', 'so then i said them we are confident about our buy recommendation on that stock\\n', 28500)\n",
      "('jealous-girl.jpg', 'you want me to shave my bush why to make it look like your ex\\n', 28690)\n",
      "('you-dont-say-meme.jpg', \"mario uses a mushroom? you don't say!\\n\", 28880)\n",
      "('lame-pun-coon.jpg', \"did you hear about the mom who injected her beauty pageant kid with botox? when the kid didn't win, she didn't look surprised.\\n\", 29070)\n",
      "('dr-evil-quote.jpg', ' platonic\\n', 29260)\n",
      "('yaowonkaxd.jpg', 'nice joke  bro\\n', 29450)\n",
      "('es-bakans.jpg', 'ooooh my gud lol guies in so exicted\\n', 29640)\n",
      "('bill-oreilly-proves-god.jpg', 'actually actually actually actually actually\\n', 29830)\n",
      "('skyrim-stan.jpg', 'i have enough gold to pay the bounty of killing your entire town just something to think about\\n', 30020)\n",
      "('bane.jpg', 'when the cooking oil flashes you have my permission to fry\\n', 30210)\n",
      "('donald-trump.jpg', 'happy birthday kathy! from president donald trump\\n', 30400)\n",
      "('trologirl.jpg', \"i'm wet from that water balloon fight\\n\", 30590)\n",
      "('ptsd-karate-kyle.jpg', 'ryan is dead my name is ryu.\\n', 30780)\n",
      "('look-marge.jpg', 'look marge im a bass player\\n', 30970)\n",
      "('aysi.jpg', 'ay si whiskey river because wildwest is full of frontera people!\\n', 31160)\n",
      "('fuck-that-guy.jpg', 'i live  in a trailer\\n', 31350)\n",
      "('overly-manlyman.jpg', 'house key? you mean gun?\\n', 31540)\n",
      "('look-at-all-the-fucks-i-give.jpg', \"look at all the pussy i can't have\\n\", 31730)\n",
      "('fps-n00b.jpg', 'yeh man i had a ps4 grapics was shit\\n', 31920)\n",
      "('racist-dog.jpg', \"i don't hate niggers i think everyone should own one\\n\", 32110)\n",
      "('picard-facepalm.jpg', \"i'll have the veggie plate lunch with the fish, macncheese and the cobbler\\n\", 32300)\n",
      "('pennywise-the-clown.jpg', 'i hope you slept well christine? i was under your bed!\\n', 32490)\n",
      "('pulp-fiction.jpg', 'did you just put that tuna can in the regular bin?\\n', 32680)\n",
      "('cool-dog.jpg', 'hey mate finished homework already?\\n', 32870)\n",
      "('thats-a-paddlin.jpg', \"installing cpanel? that's a paddlin'\\n\", 33060)\n",
      "('in-soviet-russia.jpg', 'in soviet russia you are russain\\n', 33250)\n",
      "('you-shall-not-pass.jpg', 'if you dont turn in homework you shall not pass!!\\n', 33440)\n",
      "('crying-face.jpg', \"when it's valentines day  and you are lonely\\n\", 33630)\n",
      "('journalist.jpg', 'when someone who ignores you in person \"likes\" the funny pic you posted  and you\\'re like \"bitch please! you\\'re not allowed to like that\"\\n', 33820)\n",
      "('your-country-needs-you.jpg', 'your governing body needs you\\n', 34010)\n",
      "('bane-meme.jpg', 'dark humour is like a kid with cancer it never gets old\\n', 34200)\n",
      "('computer-kid.jpg', '20 bucks\\n', 34390)\n",
      "('blackjack-and-hookers-bender.jpg', \"no guest artist for dyekte? fine, we'll create our own special guest appearances with blackjack and timon domela and cheryl jubitana\\n\", 34580)\n",
      "('omsk-crow.jpg', 'tony banks\\n', 34770)\n",
      "('obama-laughing.jpg', 'then the gop asked, \"why won\\'t you work with us?\" and i replied, \" cause you\\'re not muslim.\"\\n', 34960)\n",
      "('racist-dawg.jpg', 'me racist? no, i love black people almost as much as they love watermelons\\n', 35150)\n",
      "('crazy-camel-lol.jpg', 'i beleev in mo hammed\\n', 35340)\n",
      "('i-have-no-idea-what-im-doing-dog-with-tie.jpg', 'turns out i suck at memes \\n', 35530)\n",
      "('idiot-nerd-girl.jpg', 'omg! i love batman! dark knight? whos that?\\n', 35720)\n",
      "('romneymakescom.jpg', 'i support one direction\\n', 35910)\n",
      "('annoying-gamer-kid.jpg', 'playes dead island thinks he can survive a zombie apocalypse\\n', 36100)\n",
      "('mafia-baby.jpg', 'what the fuck is going on here?\\n', 36290)\n",
      "('evil-raccoon.jpg', 'too much time hatching plan, forgot to feed kids\\n', 36480)\n",
      "('guess-who-you.jpg', 'guess who my  wcw is \\n', 36670)\n",
      "('uncle-dolan.jpg', 'nxt tme wee it at mc dolan pls\\n', 36860)\n",
      "('technologically-impaired-duck.jpg', 'purchase a new mac buy antivirus and microsoft office\\n', 37050)\n",
      "('roleplaying-rabbit.jpg', 'rp smut fantasize later\\n', 37240)\n",
      "('flniuydl.jpg', 'instagram hastag only in amsterdam\\n', 37430)\n",
      "('you-keep-using-that-word-i-dont-think-it-means-what-you-think-it-means.jpg', '\"mary sue\"  you keep using that word. i don\\'t think it means what you think it means\\n', 37620)\n",
      "('pepperidge-farm-remembers-fg.jpg', 'remeber when the clintons were honest then you will like what the horse just put in the bag\\n', 37810)\n"
     ]
    }
   ],
   "source": [
    "#CREATE EVALUATION SET\n",
    "#SHOULD BE MEMES WITH REPEATED FORMAT \n",
    "for i in range(100,200):\n",
    "    print(data_memes[i*190].replace('/data/alpv95/MemeProject/im2txt/memes/',''),data_captions[i*190],i*190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = [0,570,760,1140,2850,3040,4940,28500]\n",
    "eval_captions = ['y u no','i dont always','not sure if','one does not simply','what if i told you','what if','so youre telling me','so then i said']\n",
    "eval_memes = []\n",
    "\n",
    "for idx in eval_examples:\n",
    "    eval_memes.append(data_memes[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385708"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Should also look at removing examples which are similar but not exactly the same\n",
    "\n",
    "c = list(zip(data_memes, data_captions))\n",
    "no_repeats = []\n",
    "# order preserving\n",
    "def idfun(x): return x\n",
    "\n",
    "seen = {}\n",
    "no_repeats = []\n",
    "for item in c:\n",
    "    marker = idfun(item[1])\n",
    "    # in old Python versions:\n",
    "    # if seen.has_key(marker)\n",
    "    # but in new ones:\n",
    "    if marker in seen: continue\n",
    "    seen[marker] = 1\n",
    "    no_repeats.append(item)\n",
    "print(len(data_captions))\n",
    "len(no_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(no_repeats)\n",
    "memes_shuffled, captions_shuffled = zip(*no_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385708\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    word_captions.append(words)\n",
    "print(len(word_captions))\n",
    "#word_captions = list(set(word_captions))\n",
    "#print(len(word_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary.\n",
      "('Total words:', 150280)\n",
      "('Words in vocabulary:', 41153)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Creating vocabulary.\")\n",
    "counter = Counter()\n",
    "for c in word_captions:\n",
    "    counter.update(c)\n",
    "print(\"Total words:\", len(counter))\n",
    "\n",
    "# Filter uncommon words and sort by descending count.\n",
    "word_counts = [x for x in counter.items() if x[1] >= 3]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Words in vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionary.\n",
    "reverse_vocab = [x[0] for x in word_counts]\n",
    "#unk_id = len(reverse_vocab)\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10346"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['aaaaaaaand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "38520\n",
      "('Wrote vocabulary file:', 'vocab3.txt')\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION=300 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = current_dir\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "index_counter = 0\n",
    "\n",
    "with open('glove.42B.300d.txt','r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        if word in vocab_dict:\n",
    "            index_counter += 1\n",
    "            word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "            word2idx[word] = index_counter # PAD is our zeroth index so shift by one\n",
    "            weights.append(word_weights)\n",
    "        if index % 100000 == 0:\n",
    "            print(index)\n",
    "        if index + 1 == 2000000:\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random #maybe include start and end token here\n",
    "UNKNOWN_TOKEN=len(weights)\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN\n",
    "word2idx['<S>'] = UNKNOWN_TOKEN + 1\n",
    "word2idx['</S>'] = UNKNOWN_TOKEN + 2\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "#Save Vocabulary\n",
    "with tf.gfile.FastGFile('vocab4.txt', \"w\") as f:\n",
    "    f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word2idx.iteritems()]))\n",
    "print(\"Wrote vocabulary file:\", 'vocab3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44847  , -0.039212 ,  0.10393  , -0.18311  , -0.22794  ,\n",
       "        0.071043 ,  0.68074  , -1.0702   ,  0.10879  ,  0.32581  ,\n",
       "       -0.0075187, -0.7147   ,  0.38525  , -0.035042 ,  0.084247 ,\n",
       "        0.21304  ,  0.025578 , -0.38933  , -0.43553  , -0.17755  ,\n",
       "        0.4037   , -0.10913  , -0.056811 , -0.044236 , -0.12161  ,\n",
       "       -0.37139  , -0.3668   , -0.78518  ,  0.1406   ,  0.41383  ,\n",
       "       -0.06213  ,  0.64063  , -0.43111  , -0.16398  ,  0.15145  ,\n",
       "        0.51573  ,  0.39917  , -0.4754   ,  0.24798  , -0.13286  ,\n",
       "        0.30063  , -0.077097 , -0.064587 ,  0.30917  ,  0.65839  ,\n",
       "        0.0059184,  0.10049  ,  0.34199  ,  0.16988  ,  0.15465  ,\n",
       "        0.2108   , -0.16124  ,  0.16009  , -0.058738 ,  0.0602   ,\n",
       "       -0.4676   , -0.41398  ,  0.13282  ,  0.34347  ,  0.31409  ,\n",
       "       -0.2003   , -0.18261  ,  0.28005  ,  0.37784  , -0.085211 ,\n",
       "       -0.82782  , -0.19752  , -0.16085  , -0.051853 , -0.046978 ,\n",
       "       -0.090662 , -0.50244  ,  0.24386  ,  0.29688  , -0.2001   ,\n",
       "        0.19445  , -0.28373  , -0.18313  , -0.39333  ,  0.065854 ,\n",
       "       -0.011839 , -0.23055  , -0.43664  ,  0.080693 , -0.26789  ,\n",
       "       -0.3679   , -0.35613  ,  0.056707 , -0.20576  ,  0.17426  ,\n",
       "       -0.22824  ,  0.15239  , -0.056002 , -0.0097033, -0.30139  ,\n",
       "        0.14433  ,  0.40497  ,  0.054102 ,  0.68279  ,  0.14556  ,\n",
       "        0.0012223,  0.35829  , -0.1578   , -0.26101  , -0.19565  ,\n",
       "        0.10731  ,  0.049116 , -0.25991  ,  0.20451  ,  0.37388  ,\n",
       "        0.091763 ,  0.01326  , -0.16377  , -0.058506 , -0.21469  ,\n",
       "        0.060703 ,  0.16069  , -0.59625  , -0.039471 , -0.17519  ,\n",
       "        0.17627  , -0.35163  , -0.13807  ,  0.18959  , -0.33912  ,\n",
       "       -0.017737 ,  0.2403   ,  0.48794  ,  0.090305 ,  0.24129  ,\n",
       "        0.1004   ,  0.14941  , -0.040279 , -0.23966  , -0.010672 ,\n",
       "       -0.14114  ,  0.25642  , -0.29952  , -0.15572  , -0.66248  ,\n",
       "        0.17192  , -0.20204  ,  0.24225  ,  0.05396  ,  0.08926  ,\n",
       "        0.12152  , -0.3615   ,  0.15428  ,  0.27732  , -0.089112 ,\n",
       "        0.31128  , -0.14316  ,  0.18377  , -0.10577  ,  0.5478   ,\n",
       "        0.12911  ,  0.47722  , -0.50255  ,  0.12407  , -0.060889 ,\n",
       "        0.11583  ,  0.22377  , -0.39128  , -0.097678 , -0.16674  ,\n",
       "       -0.50881  ,  0.27624  , -0.39481  ,  0.17492  ,  0.63578  ,\n",
       "       -0.15305  ,  0.16731  , -0.030143 ,  0.35387  ,  0.18817  ,\n",
       "       -0.52209  , -0.13349  , -0.2518   , -0.51798  ,  0.62682  ,\n",
       "        0.060017 , -0.041401 , -0.028021 ,  0.11233  ,  0.11447  ,\n",
       "        0.44617  , -0.032094 , -0.052696 , -0.038342 ,  0.3574   ,\n",
       "        0.30677  , -0.51606  ,  0.16839  , -0.024237 ,  0.22719  ,\n",
       "        0.018605 ,  0.45249  ,  0.26825  ,  0.263    ,  0.40592  ,\n",
       "       -0.44148  , -0.060587 , -0.25607  , -0.44626  , -0.10527  ,\n",
       "       -0.66607  ,  0.48883  , -0.54111  ,  0.1403   , -0.0476   ,\n",
       "       -0.20509  , -0.36993  ,  0.15421  , -0.093329 ,  0.4347   ,\n",
       "        0.14337  ,  0.052247 , -0.17844  , -0.1292   , -0.18123  ,\n",
       "        0.15523  , -0.14887  , -0.058583 ,  0.40293  ,  1.0223   ,\n",
       "        0.0631   , -0.28373  , -0.14486  , -0.014079 , -0.4358   ,\n",
       "       -0.46028  , -0.70722  ,  0.69638  , -0.035604 , -0.18943  ,\n",
       "       -0.30317  ,  0.080057 , -0.21485  ,  0.10918  ,  0.28817  ,\n",
       "       -0.07454  , -0.079632 ,  0.452    ,  0.011182 ,  0.4781   ,\n",
       "       -0.25833  ,  0.34898  , -0.016316 ,  0.092757 , -0.37827  ,\n",
       "       -0.27047  , -0.03206  ,  0.30495  , -0.18112  , -0.023923 ,\n",
       "       -0.46283  ,  0.40218  ,  0.25496  ,  0.03829  , -0.16211  ,\n",
       "       -0.029873 , -0.033979 ,  0.16065  , -0.26032  , -0.24203  ,\n",
       "        0.12691  , -0.31384  ,  0.48393  , -0.73027  , -0.24479  ,\n",
       "       -0.30343  ,  0.19906  ,  0.12204  ,  0.38072  ,  0.509    ,\n",
       "        0.15137  , -0.52911  ,  0.065863 , -0.3758   , -0.1274   ,\n",
       "        0.080856 ,  0.11545  ,  0.40141  , -0.21268  ,  0.71381  ,\n",
       "        0.1793   ,  0.13791  , -0.37751  ,  0.24224  , -0.12526  ,\n",
       "        0.24988  , -0.39193  , -0.23447  ,  0.38872  ,  0.96019  ,\n",
       "       -0.033372 , -0.08502  , -0.374    , -0.44978  ,  0.80095  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[37984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('embedding_matrix5',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_caption.append(word2idx['</S>'])\n",
    "    token_captions.append(token_caption)\n",
    "    \n",
    "#for eval set\n",
    "eval_tokens = []\n",
    "for capt in eval_captions:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    #token_caption.append(word2idx['</S>'])\n",
    "    eval_tokens.append(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38518, 35941, 8, 38517, 636, 636, 1588, 16867, 104, 292, 6059, 4334, 4635, 38517, 38519]\n",
      "i'm in alask oh oh fuck spammers here put anti spam mask*\n",
      "\n",
      "[38518, 46, 137, 31, 716]\n",
      "one does not simply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38517"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_captions[330000])\n",
    "print(captions_shuffled[330000])\n",
    "print(eval_tokens[3])\n",
    "print(eval_captions[3])\n",
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054\n",
      "42223\n",
      "42438\n",
      "106463\n",
      "157358\n",
      "209285\n",
      "242584\n",
      "276409\n",
      "284211\n",
      "343892\n",
      "370946\n",
      "379445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,ting in enumerate(deleters):\n",
    "    print(ting)\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)\n",
    "len(deleters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31996\n"
     ]
    }
   ],
   "source": [
    "many_unk = []\n",
    "for i,capt in enumerate(token_captions):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        many_unk.append(i)\n",
    "print(len(many_unk))\n",
    "\n",
    "for i,ting in enumerate(many_unk):\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353700"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memes_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "def _bytes_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n",
    "\n",
    "def _floats_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR ALEXNET \n",
    "memes_shuffled_int = []\n",
    "for i,meme in enumerate(memes_shuffled):\n",
    "    memes_shuffled_int.append(np.int_(meme*1000000000000000))\n",
    "print(memes_shuffled_int[20][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR INCEPTION\n",
    "class ImageDecoder(object):\n",
    "    \"\"\"Helper class for decoding images in TensorFlow.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a single TensorFlow Session for all image decoding calls.\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "        # TensorFlow ops for JPEG decoding.\n",
    "        self._encoded_jpeg = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._encoded_jpeg, channels=3)\n",
    "\n",
    "    def decode_jpeg(self, encoded_jpeg):\n",
    "        image = self._sess.run(self._decode_jpeg,\n",
    "                               feed_dict={self._encoded_jpeg: encoded_jpeg})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING TRAINING SET (ALEXNET)\n",
    "import sys\n",
    "train_filename = 'train.tfrecords6'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(memes_shuffled_int[i].tostring()), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/353700\n",
      "Train data: 20000/353700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-66fd11457912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_jpeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Skipping file with invalid JPEG data: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-7cba5ef93277>\u001b[0m in \u001b[0;36mdecode_jpeg\u001b[0;34m(self, encoded_jpeg)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_jpeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_jpeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         image = self._sess.run(self._decode_jpeg,\n\u001b[0;32m---> 14\u001b[0;31m                                feed_dict={self._encoded_jpeg: encoded_jpeg})\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peirson/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peirson/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peirson/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peirson/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peirson/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SAVING TRAINING SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'train.tfrecords7'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(memes_shuffled[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING EVAL SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'eval.tfrecords7'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(eval_memes)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(eval_memes[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(eval_memes))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(eval_tokens[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38518, 31, 267, 45]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,wp in enumerate(x):\n",
    "    if wp[0] == 1:\n",
    "        del x[i]\n",
    "x = x[0:2]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile('vocab2.txt', mode=\"r\") as f:\n",
    "      wordidx_pairs = list(f.readlines())\n",
    "wordidx_pairs = [(line.split()[0],line.split()[1]) for line in wordidx_pairs]\n",
    "vocab = dict([(x, int(y)) for (x, y) in wordidx_pairs]) #changed this to reflect vocab.txt format\n",
    "x = sorted(vocab.iteritems(), key=lambda x: int(x[1]))\n",
    "reverse_vocab = [y[0] for y in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vocab[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
