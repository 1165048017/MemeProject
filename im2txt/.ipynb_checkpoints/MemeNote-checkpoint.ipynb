{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some basic imports and setups\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#mean of imagenet dataset in BGR\n",
    "imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "image_dir = os.path.join(current_dir, 'memes')\n",
    "#image_dir = current_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "\n",
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000,[],['fc7','fc8'],512,weights_path='/data/alpv95/MemeProject/im2txt/bvlc_alexnet.npy') #maybe need to put fc8 in skip_layers\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc6\n",
    "\n",
    "#create op to calculate softmax \n",
    "#softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Converting captions and meme vector representations into single Tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires putting memes through alexnet to find their vector rep, shuffling the captions, changing captions into their word2idx, finally saving one caption together with one meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194)\n",
      "(100, 16803, 16803)\n",
      "sizing error\n",
      "(200, 35980, 35980)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(300, 55039, 55039)\n",
      "sizing error\n",
      "sizing error\n",
      "(400, 74059, 74059)\n",
      "sizing error\n",
      "sizing error\n",
      "(500, 92625, 92625)\n",
      "sizing error\n",
      "sizing error\n",
      "(600, 111107, 111107)\n",
      "sizing error\n",
      "(700, 129491, 129491)\n",
      "sizing error\n",
      "(800, 147619, 147619)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(900, 165654, 165654)\n",
      "(1000, 183779, 183779)\n",
      "sizing error\n",
      "(1100, 201898, 201898)\n",
      "(1200, 218713, 218713)\n",
      "sizing error\n",
      "(1300, 236436, 236436)\n",
      "sizing error\n",
      "sizing error\n",
      "(1400, 253019, 253019)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(1500, 269141, 269141)\n",
      "(1600, 285583, 285583)\n",
      "sizing error\n",
      "sizing error\n",
      "(1700, 301230, 301230)\n",
      "sizing error\n",
      "(1800, 316022, 316022)\n",
      "sizing error\n",
      "(1900, 331101, 331101)\n",
      "(2000, 346213, 346213)\n",
      "sizing error\n",
      "sizing error\n",
      "(2100, 362034, 362034)\n",
      "(2200, 375316, 375316)\n",
      "sizing error\n",
      "sizing error\n",
      "(2300, 389635, 389635)\n",
      "sizing error\n",
      "sizing error\n",
      "(2400, 402617, 402617)\n",
      "(2500, 414549, 414549)\n"
     ]
    }
   ],
   "source": [
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "counter = 0\n",
    "\n",
    "#Doing everything in one script: (the fc6 vectors are quite sparse)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    for i,meme in enumerate(img_files):\n",
    "        #meme_name = meme.replace('/Users/ALP/Desktop/Stanford/CS224n/MemeProject/memes/','')\n",
    "        #meme_name = meme_name.replace('.jpg','').lower()\n",
    "        #meme_name = meme_name.replace('-',' ')\n",
    "        img = Image.open(meme)\n",
    "        try:\n",
    "            img.thumbnail((227, 227), Image.ANTIALIAS)\n",
    "            #img = img.resize((227,227))\n",
    "            #use img.thumbnail for square images, img.resize for non square\n",
    "            assert np.shape(img) == (227, 227, 3)\n",
    "        except AssertionError:\n",
    "            img = img.resize((227,227))\n",
    "            print('sizing error')\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img = img - imagenet_mean #should probably change this\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "\n",
    "        meme_vector = sess.run(score, feed_dict={x: img, keep_prob: 1}) #[1,4096]\n",
    "        meme_vector = np.reshape(meme_vector,[4096])\n",
    "        assert np.shape(meme_vector) == (4096,)\n",
    "        #match = [s.split('-',1)[-1].lstrip() for s in captions if meme_name in s]\n",
    "        \n",
    "        match = []\n",
    "        meme_name = captions[counter].split(' - ')[0]\n",
    "        \n",
    "        while meme_name in captions[counter]:\n",
    "                if counter==len(captions)-1:\n",
    "                    match.append(captions[counter].split(' - ')[-1])\n",
    "                    break\n",
    "                elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    match.append(captions[counter].split(' - ')[-1])\n",
    "                    counter += 1\n",
    "                \n",
    "        \n",
    "        #now save in tfrecords format, or prepare for that action\n",
    "        meme_vectors = [meme_vector for cap in match]\n",
    "        assert len(meme_vectors) == len(match)\n",
    "        data_memes.extend(meme_vectors)\n",
    "        data_captions.extend(match)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i,len(data_memes),len(data_captions))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385823"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = list(zip(data_memes, data_captions))\n",
    "no_repeats = []\n",
    "# order preserving\n",
    "def idfun(x): return x\n",
    "\n",
    "seen = {}\n",
    "no_repeats = []\n",
    "for item in c:\n",
    "    marker = idfun(item[1])\n",
    "    # in old Python versions:\n",
    "    # if seen.has_key(marker)\n",
    "    # but in new ones:\n",
    "    if marker in seen: continue\n",
    "    seen[marker] = 1\n",
    "    no_repeats.append(item)\n",
    "print(len(data_captions))\n",
    "len(no_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(no_repeats)\n",
    "memes_shuffled, captions_shuffled = zip(*no_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calebs the name dicks is the game\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_shuffled[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385823\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    word_captions.append(words)\n",
    "print(len(word_captions))\n",
    "#word_captions = list(set(word_captions))\n",
    "#print(len(word_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary.\n",
      "('Total words:', 150310)\n",
      "('Words in vocabulary:', 41160)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Creating vocabulary.\")\n",
    "counter = Counter()\n",
    "for c in word_captions:\n",
    "    counter.update(c)\n",
    "print(\"Total words:\", len(counter))\n",
    "\n",
    "# Filter uncommon words and sort by descending count.\n",
    "word_counts = [x for x in counter.items() if x[1] >= 3]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Words in vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionary.\n",
    "reverse_vocab = [x[0] for x in word_counts]\n",
    "#unk_id = len(reverse_vocab)\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'withdrawl'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab[41140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "38525\n",
      "('Wrote vocabulary file:', 'vocab2.txt')\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION=300 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = current_dir\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "index_counter = 0\n",
    "\n",
    "with open('glove.42B.300d.txt','r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        if word in vocab_dict:\n",
    "            index_counter += 1\n",
    "            word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "            word2idx[word] = index_counter # PAD is our zeroth index so shift by one\n",
    "            weights.append(word_weights)\n",
    "        if index % 100000 == 0:\n",
    "            print(index)\n",
    "        if index + 1 == 2000000:\n",
    "            # Limit vocabulary to top 40k terms\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random #maybe include start and end token here\n",
    "UNKNOWN_TOKEN=len(weights)\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN\n",
    "word2idx['<S>'] = UNKNOWN_TOKEN + 1\n",
    "word2idx['</S>'] = UNKNOWN_TOKEN + 2\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "#Save Vocabulary\n",
    "with tf.gfile.FastGFile('vocab2.txt', \"w\") as f:\n",
    "    f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word2idx.iteritems()]))\n",
    "print(\"Wrote vocabulary file:\", 'vocab2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.0441e-01, -1.6302e-01, -2.7849e-01,  2.3274e-01, -3.7061e-01,\n",
       "       -6.8627e-03,  9.7021e-01, -3.4582e-01,  2.8243e-01,  6.5861e-03,\n",
       "       -1.8000e-01, -3.4418e-01,  4.3996e-02,  2.8602e-01,  2.8923e-01,\n",
       "        4.8672e-01,  2.6560e-01,  3.2488e-01, -1.0822e-01, -1.5412e-01,\n",
       "        1.2611e-01,  1.7512e-01, -1.5990e-01, -1.1308e-01, -3.1108e-01,\n",
       "       -1.9836e-02,  1.8696e-01, -1.8816e-01, -3.5828e-02, -8.9612e-02,\n",
       "        4.5423e-01, -1.4298e-01,  1.4402e-01, -5.8635e-02,  5.2800e-02,\n",
       "       -1.0348e-01,  1.2405e-03,  4.1776e-01,  1.2423e-01,  1.2829e-01,\n",
       "       -1.3216e-01,  1.7993e-01,  4.6104e-02, -1.3071e-01,  2.7108e-01,\n",
       "        3.2288e-01,  3.1108e-01,  1.7780e-01,  5.2920e-01,  8.3725e-02,\n",
       "        1.2514e-01, -1.0971e-01,  3.5899e-01, -9.1469e-02,  9.1353e-02,\n",
       "        1.0058e-01,  1.8793e-01,  1.5856e-02, -1.3132e-01, -2.4476e-01,\n",
       "        8.1542e-02, -1.1140e-01, -1.9248e-01, -1.5279e-01, -2.9478e-01,\n",
       "        4.4122e-01, -2.4848e-01,  3.5278e-01, -1.9929e-01,  6.8212e-01,\n",
       "       -6.1724e-02, -5.2791e-01, -2.0018e-01, -1.1943e-01,  4.1279e-01,\n",
       "        3.1404e-02, -5.5947e-01, -4.2564e-01,  1.7867e-01,  2.8153e-01,\n",
       "        7.0408e-02,  3.5273e-03, -3.8720e-02,  1.7424e-01, -3.2138e-02,\n",
       "        1.4311e-01, -1.0281e-01,  1.3749e-01,  1.1467e-01, -2.6971e-01,\n",
       "       -5.1020e-02,  1.5470e-01,  2.3803e-01, -4.2277e-01,  5.3056e-02,\n",
       "       -3.5572e-02,  3.1207e-01, -3.5892e-01,  2.6151e-01, -3.8367e-01,\n",
       "        5.7914e-01,  1.5915e-01, -1.9963e-01,  5.7370e-02,  2.8964e-01,\n",
       "        8.0832e-02, -2.4848e-01,  2.5728e-01, -3.0661e-01, -1.9063e-01,\n",
       "        3.2489e-01, -7.8869e-02,  1.3844e-01,  2.0372e-01,  1.9668e-01,\n",
       "        2.2629e-01, -2.0642e-01,  2.3612e-02, -9.2083e-02,  6.5409e-01,\n",
       "        4.4793e-01,  1.8819e-02,  1.0903e-01,  3.4159e-02, -2.6973e-02,\n",
       "        3.1093e-01,  3.6340e-01,  7.1025e-02,  7.7231e-02,  3.2448e-01,\n",
       "       -3.8182e-01,  1.5139e-01,  5.0292e-01,  1.4633e-01,  1.5822e-01,\n",
       "        2.5821e-01,  2.7119e-01,  1.8608e-01, -1.0747e-02, -1.4901e-01,\n",
       "        4.4913e-02, -3.6554e-01, -1.4200e-01,  2.8998e-01,  2.3692e-01,\n",
       "        2.5520e-01, -1.2524e-01, -2.0565e-01, -6.6669e-02, -3.8778e-01,\n",
       "       -1.5802e-01,  5.9623e-02, -3.9620e-01, -3.0199e-01, -2.1437e-01,\n",
       "        1.6188e-01,  2.5731e-01,  1.3777e-01,  1.0122e-01,  9.3458e-02,\n",
       "       -7.1721e-02,  1.5313e-01,  8.5681e-02,  1.0079e-01,  3.1919e-02,\n",
       "        3.9416e-01, -5.3358e-01,  1.8380e-01,  7.7135e-02,  5.1948e-01,\n",
       "        1.3742e-03, -5.8574e-01,  8.1124e-02,  2.8009e-01,  6.5667e-01,\n",
       "       -4.9229e-02,  1.0068e-02,  6.6914e-01,  1.7488e-01,  1.2984e-01,\n",
       "       -8.6165e-02,  7.3057e-02, -6.8759e-02, -5.9036e-01,  2.4840e-01,\n",
       "       -6.0310e-01,  1.5383e-01, -9.1507e-02,  1.0054e-01,  2.9150e-01,\n",
       "       -5.5150e-01,  1.6952e-01,  1.1411e-02, -4.5516e-01, -9.4548e-02,\n",
       "       -2.1145e-01,  8.2199e-03,  9.6234e-02, -9.2130e-02,  2.8587e-01,\n",
       "        2.8321e-01,  1.7657e-01,  1.6638e-01,  3.1004e-01, -1.0221e-01,\n",
       "       -4.0929e-01, -1.8109e-01, -2.4930e-01,  1.9178e-01,  5.5093e-01,\n",
       "       -2.1769e-01,  5.1056e-01,  1.2851e-01,  1.2170e-01,  5.3515e-01,\n",
       "        2.7630e-01,  2.7235e-01, -3.5832e-01, -1.1626e-01, -4.5992e-01,\n",
       "        6.0116e-02,  8.1075e-02,  5.2265e-01,  1.5686e-01,  2.7701e-01,\n",
       "       -4.3148e-01,  5.0267e-02,  2.1897e-03,  7.4896e-02,  2.8618e-01,\n",
       "       -1.7851e-01,  1.3370e-01,  7.2439e-02, -2.8975e-01,  4.0431e-01,\n",
       "        6.4650e-02, -1.2186e-01, -4.9694e-01,  3.8002e-01,  2.1155e-01,\n",
       "       -1.5395e-01, -8.3100e-02,  3.1965e-01, -3.9631e-01, -5.0182e-01,\n",
       "       -1.7555e-02,  1.7623e-01,  1.7904e-03, -7.7584e-03,  7.7555e-02,\n",
       "       -4.5904e-01, -4.7067e-01,  8.4898e-03, -1.4789e-01,  1.5973e-01,\n",
       "       -3.0488e-01, -1.8265e-01,  7.0750e-01, -9.1312e-02,  2.5699e-01,\n",
       "       -3.3206e-02,  2.5934e-01, -1.3471e-01,  1.4732e-02, -4.0559e-01,\n",
       "       -1.7067e-01,  4.0978e-01, -2.0366e-02, -2.9717e-01, -7.8739e-02,\n",
       "        9.8380e-02,  1.5140e-01, -7.7005e-02,  2.0799e-02, -9.1767e-02,\n",
       "       -1.5563e-01, -7.6407e-03,  2.4072e-01,  2.7340e-01,  1.1457e-01,\n",
       "       -4.6978e-01,  3.4456e-01, -3.7141e-01,  3.4213e-01,  2.5071e-01,\n",
       "       -1.1188e-01, -4.0498e-01,  2.7028e-01,  5.0829e-02,  6.2754e-02,\n",
       "        3.5683e-01,  3.6338e-01,  5.6807e-02, -3.3002e-01,  7.3750e-02,\n",
       "        4.2033e-04,  3.3003e-01, -4.0811e-01, -1.8448e-01, -6.2080e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[37984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('embedding_matrix3',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_caption.append(word2idx['</S>'])\n",
    "    token_captions.append(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38523, 4, 37847, 193, 32, 36481, 1529, 5, 2, 15070, 25334, 38524]\n",
      "and that's why we don't listen to the extinct mayans\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38522"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_captions[330000])\n",
    "print(captions_shuffled[330000])\n",
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-              -\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)\n",
    "captions_shuffled[deleters[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ting in enumerate(deleters):\n",
    "    del captions_shuffled[ting-i]\n",
    "    del meme_shuffled[ting-i]\n",
    "    del token_captions[ting-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "c = list(zip(data_memes, token_captions))\n",
    "shuffle(c)\n",
    "memes_shuffled, captions_shuffled = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "def _bytes_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n",
    "\n",
    "def _floats_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_shuffled_int = []\n",
    "for i,meme in enumerate(memes_shuffled):\n",
    "    memes_shuffled_int.append(np.int_(meme*1000000000))\n",
    "print(memes_shuffled_int[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(captions_shuffled[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_sequence_example(image, decoder, vocab):\n",
    "    \"\"\"Builds a SequenceExample proto for an image-caption pair.\n",
    "    Args:\n",
    "        image: An ImageMetadata object.\n",
    "        decoder: An ImageDecoder object.\n",
    "        vocab: A Vocabulary object.\n",
    "      Returns:\n",
    "        A SequenceExample proto.\n",
    "      \"\"\"\n",
    "    with tf.gfile.FastGFile(image.filename, \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        return\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"image/image_id\": _int64_feature(image.image_id),\n",
    "          \"image/data\": _bytes_feature(encoded_image),\n",
    "      })\n",
    "    \n",
    "    assert len(image.captions) == 1\n",
    "    caption = image.captions[0]\n",
    "    caption_ids = [vocab.word_to_id(word) for word in caption]\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"image/caption\": _bytes_feature_list(caption),\n",
    "          \"image/caption_ids\": _int64_feature_list(caption_ids)\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    return sequence_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "train_filename = 'train.tfrecords4'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(memes_shuffled_int[i].tostring()),  #this is the part that needs to be a float save\n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(captions_shuffled[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
